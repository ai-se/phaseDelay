 \section*{Response to Reviewers}

 
{\bf Editor's comments}
 
Dear authors,

Three knowledgeable reviewers have scrutinized the manuscript
and found it interesting and relevant,  although in need for
certain improvements. Therefore, the authors are
 invited to submit a revised version for a new review, where
 the reviewers' comments
are addressed.

\begin{quote} {\em Thank you for the careful reviews. Using
those comments we have extensively revised and extended this draft.
We trust you will find this draft gratefully improved}
\end{quote}

Particularly, this holds for:


1) the connection to activities of theory building in ESE
(reviewers 1).  


 

\begin{quote}{\em This new draft  now maps our work into the SE literature on theory building.}\end{quote}
 
2) clarification about the data origin and
characteristics (reviewer 1). 
 

\begin{quote}{\em
This new draft  now maps not contains extensive notes on the origin and characteristics of our data.}\end{quote}

3)  use of statistical analysis (reviewers 2 and 3) 


\begin{quote}{\em All our results are now augmented with statistical significance tests as well as effect size tests. Those tests greatly strength the overall message of this paper-- that there is no significant
phase delay effect.}\end{quote}

 
- writing style, that would benefit form being more
precise (reviewer 3)

\begin{quote}{\em We have carefully read those comments  nd have made numerous changes
as directed by reviewer3.}\end{quote}

{\bf Reviewer \#1's Comments}


The paper investigates whether a widely
accepted "rule" in software engineering holds by looking at
a large set of empirical data. The research topic is highly
relevant and the results are interesting.   The discussion
on reassessing old truism fits the scope of the journal very
well.  The language in the paper is excellent, and the
structure is (although non-standard) easy to follow.

\begin{quote}{\em Thank you for those kind comments.}\end{quote}

It is not easy for the reader to understand what parts
of the "life of an issue" the authors claim are affected by
DIE, and it is also not too easy to understand what data the
authors have available.  

\begin{quote}{\em  We have added an example of the DIE in the Introduction, and more precisely defined how we measure this effect Section 6.\footnote{Lucas- where exactly?} }\end{quote}
 
 The authors do not relate their reassessment of an
"old truism" to the growing set of papers on theory building
in software engineering. With access to such large amounts
of empirical evidence, it should be possible to take some
steps toward improved DIE theories. 

\begin{quote}{\em  fshull XXX }\end{quote}

The different projects studied are all grouped together. I
believe studying clusters of development contexts would be
highly interesting, but unfortunately the current
characterization of projects is inadequate.  

\begin{quote}{\em  We agree with this comment and so took two actions for this draft-- one that was effective and one that was ineffective. 

Firstly, this draft now contains extensive notes on the different contexts of the
projects studied here. Those notes are offered in \tion{data_character} and \tion{171projects}.

Secondly (and this was the ineffective step), we  searched the numerous   listed in \tion{data_character} and \tion{171projects}
in order to check if any stratification contained the delayed issue effect. We failed to find a stratification
containing an exponential expansion in the time to fix issues. The reason for this was due to the dataL those
stratifications resulted in smaller sample sizes that exacerbated the large IQR's seen in this data
(to see these large IQRs,  please consider the 50th and IQR columns of Figure~\ref{fig:fix}.IQR is the
range 75th to 25th in the data. Note that most of our IQRs were larger than tan the 50th percentile; i.e. software
data exhibits large variances-- where are exacerbated by the smaller samples seen in the stratifications
 }\end{quote}



DETAILED REVIEW: The authors should expand on what they
include in the "delayed issue effect" (DIE), in particular
in relation to standard milestones in the life of an issue
and the corresponding timespans.  I'm particularly
interested in whether all "indirect" issue resolution steps
are covered, e.g., change impact analysis, updating
documentation, and regression testing. The effort involved
in these steps clearly depends on the development context of
the specific project. The short a-c) list on page 14:50
suggest that the issues studied are restricted to minor
fixes, i.e., no updates to architecture, changed hardware,
recertification, updated user manuals etc.

\begin{quote} {\em

We understand your comment here to be ``are we just reporting trivial errors?''
As shown in \fig{dtypes}, it is true that around a quarter of the fixes were simple documentation changes. That said, 75\% of the changes are quote  elaborate;   e.g. fixes to function necessitates a careful reflection of the purpose of the code.
}\end{quote}


Section 6.3 describes the data, but some aspects
should be further explained.

\begin{quote} {\em
At your suggestion, we had added extensive notes on the nature of our data--
see \tion{data_character} and \tion{171projects}.
}\end{quote}


Logging interruption time
sounds very useful, but I wonder how carefully the
developers actually did this 
 it must be really hard to keep up the discipline required.
 If you are interrupted as a developer (e.g., phone calls,
 urgent mail, someone asks a question) I don't think the
 first thing you do is to stop a timer on your computer.
 Moreover, developers often work on several defects in
 parallel, and might interweave bug fixing with new
 development. I don't think the "interruption time" captures
 all such multi-tasking, and it should at least be properly
 discussed in Section 7 "Threats to validity".

\begin{quote}{{\em 
Yes, there is undoubtedly some accuracy in the time-tracking. We provide some evidence on the TSP data set's integrity in Section 6.3, and a more extensive discussion on the issue of time-tracking and defect logging accuracy in Section 7.1 on Conclusion Validity.XXX lucas: please check the section numbers are currently
correct.
}
\end{quote}

The discussion on reassessing old truisms (Section 3) is
interesting, but it should be complemented by the
perspective of theory building in software engineering - An
active research topic lately, with a dedicated workshop
series (GTSE). I suggest looking into the following
references for a start: Sjøberg et al. (2008) "Building
Theories in Software Engineering", Smolander and Päivärinta
(2013) "Theorizing about software development practices",
and Stol and Fitzgerald (2015) "Theory-oriented software
engineering". Considering new empirical evidence is
obviously critical to theory building, and discussing your
new results in the light of an theory creation would be
valuable. The authors mention that DIE appears to occur
intermittently in certain kinds of projects - maybe the
authors could elaborate on this idea and present an improved
DIE theory based on what they now know? I believe the
authors have the best available data to do so, and I would
expect the paper to go beyond simply questioning the "old
truth". 
 
 \begin{quote}{\em 
XXX Fshull
}\end{quote}

The authors study three claims in this paper, and the
third claim is the central one: "delayed issues are not
harder to resolve". To study whether issues require longer
resolution times in later phases, the authors analyze a
large set of issue data from historical projects. Thus the
manuscript reports from an observational study rather than
an experiment with a controlled delivery of treatments.
While I believe the authors' approach is practical, I would
like to see a critical discussion on threats to validity of
observational studies (e.g., Madigan et al., A Systematic
Statistical Approach to Evaluating Evidence from
Observational Studies, Annu. Rev. Stat. Appl. 2014. 1:11-39
and Carlson and Morison, Study Design, Precision, and
Validity in Observational Studies, J Palliat Med. 2009 Jan;
12(1): 77-82). An alternative study design (although
difficult to realize in a system of industrial size) would
be to let different developers resolve the same issues for
the same software system, i.e., one group resolves an issue
during design, another during implementation, and a third
during testing - some discussion along these lines would
strengthen the validity section. 

\begin{quote}{\em 
 We have reviewed and cited the mentioned papers. We have incorporated many of the concerns raised in these papers into our expanded Section 7.4 on External Validity.
XXX lucas: please check the section numbers are currently
correct. XXX section 7.4 is a little... terse. can you add another 1/4 to 1/2 page of stuff? or is this sectionn just as large as it neeeds to be
}\end{quote}

The first and second claims are studied with much less
rigor. "DIE is a commonly held belief" is studied using a
survey of software engineers, both practitioners and
researchers. According to Fig. 2 (actually a table), the
number of respondents is 16 and 30 for practitioners and
researchers, respectively. Sixteen respondents from industry
represent a tiny survey of a very general claim that any
software engineer could respond to. Why were not more
answers collected? The authors do not have much evidence to
defend the first claim, and there is no discussion of the
corresponding validity threats. The second claim, "DIE is
poorly documented", is studied using a literature review.
Unfortunately, the method underlying the literature review
is not presented. Although it doesn't need to be an SLR of
the most rigorous kind, the authors should report how the
papers were identified. I suspect the terminology used to
describe the DIE phenomenon is highly diverse, thus it would
strengthen the paper if the authors reported how they
reviewed the literature. According to Page 19:34 only eight
publications were identified. 

\begin{quote}{\em XXX
Lucas: need to show that these were the only empirical
results for DIE in tow
}\end{quote}

The iterative fashion of modern software development,
with agile at the extreme end, is not fully discussed (the
short discussion section could be extended). The phases of
the linear development of the 80s, (such as in Fig. 1)
probably don't exist in many of the projects in the TSP
dataset, still the authors discuss the DIE-effect from the
perspective of the 80s. Page 16:31 states "DIE has been used
to justify many changes to early lifecycle software
engineering" - does this mean the agile movement
successfully mitigated DIE? This possibility is not fully
considered in the paper.
How many of the 171 projects were
(more or less) agile? This appears to be an important
characteristic of the included projects - very important to
describe! 

\begin{quote}{\em An astute observation. We provide more information on the types of projects included in our sample in Section 6.4. XXX Lucas. Check section number.
XXX Lucas: can't see the "percent agile" in 6.4. Can you work this with Bill?

More to the point -- does agile help mitigate DIE? The answer is - we don't know. A strong argument can be made that all iterative&incremental methodologies are meant to mitigate the DIE. Further, huge advances in software engineering technology and process have occurred since DIE was first observed, many of which have precisely targeted the DIE (static analysis, test-first development, automated build). Perhaps our analysis provides evidence that DIE can be defeated, at a large scale across multiple varied projects, using modern technologies. We do not yet know the causes here, but certainly it appears the DIE should not be treated as a truism any more, but rather a variable that can be controlled. We discuss this in Section 8.
}\end{quote}


Concerning characterization of the 171 projects, the
paper needs to report much more detail. I would expect to
see some descriptive statistics.   The paper does not report much
characterization of the 171 projects. I strongly suggest the
authors to dig deeper into the data, and analyzing for which
types of projects the findings hold. What patterns are there
to discover? I suggest Petersen and Wohlin, "Context in
industrial software engineering research" (ESEM2009) for
details on how to characterize development contexts.

\begin{quote}{\em Those notes are offered in \tion{data_character} and \tion{171projects}. \end{quote}




 
Fig 1: "Operation" dwarfs everything in Boehm's diagram.
Since you do not study anything post release in this paper,
I think this figure skews the reader's mental picture. If
you remove the rightmost bar and rescale accordingly, the
plot better matches the findings you report in Fig. 10. You
still identify an interesting result though, but the
presentation turns fairer. 

\begin{quote}{\em We understand why you are making that suggestion.
That figure is something of a classic. It is appropriate we chop it like that?}.
\end{quote}

MINOR COMMENTS: 
Several figures are copied from previous work. Are all copyrights properly
managed?  

\begin{quote}{\em We recently published a text on classic SE papers. We know how to manage those issues.}\end{quote}

 

\fig{raw}: Black and red is not a good pair for
grayscale printouts. Please use black and gray instead.
\begin{quote}{\em  We agree: switched to black and gray.  }\end{quote}

 
 
Some figures should be resized to better match the page width.  
 

\begin{quote}{\em  We agree: fixed. }\end{quote}


 
Page 3:14 - "The above argument" Which argument? Could be precisely specified.
 

\begin{quote}{\em  We agree: that reference has been deleted. }\end{quote}
 
Page 3:22 - Please provide the full link to the dataset.
 
\begin{quote}{\em  Our policy is to get one publication out of paper
before issueing the data in public. If this paper is accepted, we will certainly add that link. Note that we have a long history of living up to that policy-- see http://openscience.us/repo. }\end{quote}  

 
Page 18:26 - First sentence: "Unexpected results such as this one". Ambiguous reference,
please be specific.  
\begin{quote}{\em  We agree. That back reference is removed. }\end{quote}  

\begin{quote}{\em XXX Lucas. Not sure. but do you hanle all the following? }\end{quote}  

Page 18:27 - "We also survey the state of SW dev. practice /---/": given the size of the survey,
this statement feels a bit bold.

 
TYPOS ETC.  
Sec 1, §1: [3] and [30] have been swapped?  
Page 14:46 - Spell out IV\&V the first (and only) time it appears.
Page 3:24 - missing verb (is) Page 16:43 - "Did this study failed" 
Page 18:44 - Appache 
Page 19:19 - Al Ref 26 - oo --> OO

{\bf Reviewer \#2's comments}

The topic of this paper is the empirical
evaluation of a largely admitted claim in software
engineering, i.e. the exponential cost of correcting errors
according to the phase in which the errors are discovered.
The claim is first confirmed by surveying and interviewing
practitioners and experts. Next, the authors use a large set
of data from projects in SEI database. The results show
that, strictly speaking, the claim does not hold (although
some effects of delayed corrections can be noticed).

Globally, the paper is well written and the authors develop
a convincing demonstration. The topic of the paper is highly
relevant for both practitioners and people from academia.
Among many, I personally believed in this claim and had
regularly taught it in my courses. Thus, reading this paper
offers a refreshing perspective on our understandings of
software engineering background and theoretical knowledge. I
also like the suggested posture that insists on the
necessary skepticism that we should have against such
prevailing claims.

Beyond this positive global impression, I have some concerns
about the study reporting and analysis: -  It seems to me
that more descriptive statistics about the sample would
contribute to a better understanding of the scope of the
study. 

\begin{quote}{\em 
Those notes are offered in \tion{data_character} and \tion{171projects}.}\end{qupte}

I think in particular about the size (lines of codes and/or
 number of software components); a duration histogram would
 also be relevant. Accordingly, the formula for calculating
 variable "Total effort" in Fig. 7 could be explained, and
 Figure 9 made more readable (bigger size).
 
 \begin{quote}{\em We have added more description of the projects to Section 6.4. We have also added more precise definitions of our measures in Section 6.2. We have adjusted the size of figures throughout the paper to be more readable.XXX Lucas please check section references. We have added more text 
 to explain {\em effort} in \fig{dist}.}\end{quote}

 
If certain attributes for the issues and errors are
available in the data (e.g. severity, priority, etc.), they
should also be brought to the readers knowledge.  
 

\begin{quote}{\em 
Those notes are offered in \tion{data_character} and \tion{171projects}.}\end{qupte}

Figure 10 is central to the paper's demonstration; its
expressiveness could be enhanced: the formula for calculating the 50th
and 95 ratio could be provided,  the unit of column
"Percentile" could be mentioned. 

\begin{quote}{\em
\fig{raw}'s caption now has extensive explanation text. e.g. units
are added on line two. Also, an example of the calculations of the ratios is offered
in the last line of the caption. }\end{quote}

I like the discussion section, in particular, the idea
of software architectures that could be a contributing
factor for enhancing software evolution and reducing the
cost of issues and errors fixing. Is there any available
knowledge in the data set concerning any architectural
design choices made in each project?  


\begin{quote}{\em
Not explicitly, but \fig{dtypes} lists "interfaces" as a the fourth most
common type of defect. Note that "interfaces" are changed by architectural
issues. }\end{quote}
 
In the same trend of ideas, I think what has
fundamentally changed since early days of SE is the
development of requirements engineering. In actual SE
practices, problem and the solution spaces are
systematically explored thanks to RE techniques; together
with architecture, this could also explain why things have
changed since the 70'. It could be, for example, that people
tend to make less severe errors in early software project
phases (thanks to RE techniques); a similar phenomenon was
observed when SE practices become more mature (see Harter et
al. 2012).  

References Harter D. E., Kemerer C. F., Slaughter S. (2012).
Does Software Process Improvement Reduce the Severity of
Defects? A Longitudinal Field Study. IEEE Transactions on
Software Engineering, vol. 38, n° 4, p. 810-827.
```

\begin{quote}{\em
  Absolutely - there are potentially many explanations for why the DIE was NOT observed in our dataset. We discuss some more of the potential causes in Section 8.
  XXX Lucas. Is the section reference right? Do we reference Harter (and if we do, mention that here).?
 }\end{quote}

 
In the conclusion section, I feel uncomfortable with
the assertion "That data held no trace of the delayed issued
effect" (line 17). As mentioned elsewhere in the paper, the
delayed issue is not absent; it is much less significant and
systematic than what is usually claimed. Moreover, this
reduction has been demonstrated for medium sized projects;
we cannot generalize to larger projects.  
\begin{quote}{\em
  We agree -That line has been removed.
 }\end{quote}


\begin{quote}{\em
 XXX Lucas: can you retire the follwoing
 }\end{quote}

Minor remarks: - p.9, line 29 : " … reported by Shull [52], found that the cost to find certain non-critical classes of defects …" => is it the "cost to find" or the "cost to FIX"?  
```

- [X]  llayman It should be "cost to fix". We have corrected this.

 
- p.10, line 21: duplicate word "there" -
p.12, line19: missing word "One of THE guiding …" - p.16,
line 33: "distinguish" instead of "distinguished"?  - p.18,
line 42: explain acronym MEAN - p.20, line 20: useless
enumeration A1?
 

{\bf  Reviewer \#3's Comemnts}: 

I like the idea of the paper and I certainly
would like to see more empirical studies that periodically
check if our beliefs about software engineering practices
still hold (or even if they have ever held). The authors
address one of the beliefs that are more entrenched in the
population of researchers and practitioners, and a very
important one too.


\begin{quote}{\em
Thanks for those kinds words.
 }\end{quote}

At the same time, I think that the paper needs to be
improved before it can be published.

The empirical analysis (Section 6) is a bit of a let down. I
would like to see some sort of more robust and rigorous
statistical analysis, based on statistical tests.



\begin{quote}{\em
We agree. \tion{stats} lists our stats methods and \fig{raw}
shows the results (see the left-hand column: all those ``1''s mean
that our stats could not find any difference between the fix times for
delaying or not delaying.
 }\end{quote}
 
 
The paper seems to be written in a somewhat casual style,
which makes the paper more pleasant to read, but less clear
in some parts (see Detail Comments below).

\begin{quote}{\em
Thanks for those pointers. We will fix.
 }\end{quote}

DETAIL COMMENTS

Section 2

I'm not really sure what the authors mean by "We say that a
measure collected in phase 1, ., i, .. j is very much more
when that measure at phase j is larger than the sum of that
measure in earlier phases 1 <= i < j." You are defining a
property of a measure, but, the way this definition is
written, it seems as if you are defining the property of a
measure of "being much more," without any further
qualifications.  I can guess that you mean that a measure
$m$ (collected in phases, so you can denote by $m(i)$ the
value of $m$ in phase $i$) has this property *in phase $j$*
(so, it's a property m has in a specific phase and not a
general property of the measure) if $\sum_{1 <= i < i} m(i)
< m(j)$. Then, it's up to you to make this a property of the
measure, for example by using an existential "policy" ($m$
has this property if there exists one phase $j$ where
$\sum_{1 <= i < i} m(i) < m(j)$) or a universal one ($m$ has
this property if for all phases $1<j$, $\sum_{1 <= i < i}
m(i) < m(j)$).
\begin{quote}{\em
Thanks for that. We added the $m_j$ notation to Section 2, as you suggested.
 }\end{quote}
 
 
Your definition of "difficult issue" is "Issues are more
difficult when their resolution takes more time or costs
more (e.g. needs expensive debugging tools or the skills of
expensive developers)." That's not a very precise
definition, though, because it has two different
interpretations, one in terms of time and the other in terms
of effort. The reasons why you introduce this definition
become clearer only in Section 7.2, in the Threats to
Validity, and that's too late. You should move some of the
discussion of Section 7.2 here.
\begin{quote}
{\em Good idea! We did just that- see middle para in Section 2.}
\end{quote}
Why would the term "delayed issue effect" be a
generalization of the rule "requirements errors are hardest
to fix"? "Delayed" seems to be quite specific as it appears
to refer to time, while "hardest to fix" may refer to other
variables, like effort or cost.
\begin{quote}
{\em Not sure if this a reviewer requirement for something to be fixed
prior to publication, or a clarification question. We used these
terms since when we ran the survey, the audience clearly made
that generalization. Does this reply adequately to your comment?}
\end{quote}
In Claim 3, "very much more harder" sounds like a
little bit too much ... ("more harder"?)
\begin{quote}
{\em Our bad. We fixed that.}
\end{quote}
Section 4

A very minor issue: why is Figure 2 a ... figure, instead of
a table? Same for all other tables ...
\begin{quote}
{\em Authors' preference. When a document has many tables and figures, navigating
between them is hard. Calling them all "figures" makes navigation faster. 
But do you want us to change that?}
\end{quote}
Section 5.1

This is not a complete sentence "All the literature
described above that reports the onset of DIE prior to delivery.''

\begin{quote}
{\em Fixed}
\end{quote}

Section 6.3

You should rephrase your definition "a defect is any change," since a defect is what existed before the change was made and not the change itself.
\begin{quote}
{\em We now provide a more precise definition of defect in Section 6.2.
XXXX= Lucas: check section number.
}
\end{quote} 
You do not explain that "QualTest" in Fig. 8 is.
\begin{quote}
{\em Removed}
\end{quote}
The definition of "time per defect - The total # of defects found in a plan item during a removal phase divided by the total time spent on that plan item in that phase." is not correct as it is, as this would basically be the number of defects per unit time, instead. The roles of time and
defects should be reversed.
\begin{quote}{\em We now provide a more precise definition of time measurement related to defects in Section 6.2.}\end{quote}

 
Section 6.4

Besides adding a more rigorous statistical analysis, you
need improve your explanations.
\begin{quote}{\em We agree. \tion{stats} lists our stats methods and \fig{raw}
shows the results (see the left-hand column: all those ``1''s mean
that our stats could not find any difference between the fix times for
delaying or not delaying.}\end{quote}

Figure 9. The caption says "Distribution of defects found
and fixed by phase." Is that the distribution of defects
found in some phase and fixed in that phase? The text says
"The distribution of defects found and fixed per phase in
our data is shown in Figure 9. A high percentage of defects
(44\) were found and fixed in the early phases, i.e.,
requirements, high level design, and design reviews and
inspections." which doesn't add much. If that's the
distribution of defects introduced in a phase and fixed
immediately in that phase, it is not clear why you introduce
it, though.
\begin{quote}
{\em XXXX Bill: one for you.}
\end{quote}
Figure 10. The explanation of the data and histograms in the
figure is, at best, confusing. The caption says "50th and
95th percentiles of issue resolution times" and the opening
sentence of Section 6.4 says "Figure 10 shows the 50th and
95th percentile of the time spent resolving issues ..."
However, these are not "times," because you write "expressed
as ratios of resolution time in the phase where they were
first injected" a couple of paragraphs below. This seems to
be the right interpretation, but right after that you write
"The BLACK and RED bars show the increases for the 50th
(median) and 95th percentile values, respectively," which
would be a third interpretation of the data in Figure 10.
\begin{quote}
{\em \fig{raw} now has an extensive help text in the caption.
We used your comments there where we noted the ratios used in the
bard charts are unitless while the left-hand-side column numbers are
times in minutes.
}
\end{quote}
 
You need to at least mention (or, better, discuss) some of
your results, because they appear to be "counterintuitive,"
since they show that, for example, median resolution times
can even decrease if issues are not fixed immediately, but
in later phases. For example, looking at the Reqts section,
most percentages are way below 1, which seems to indicate
that some issues are actually much easier to fix at later
stages (maybe because more or better information about the
software system is available only later). That would be a
very interesting result by itself, especially if you can
provide some statistical support for it.
\begin{quote}{\em
The last paragraph of \tion{171projects} now comments
on that effect. }
\end{quote}

Section 7.1

Maybe you could analyze the projects developed in a
"traditional" way and check if there is some sort of DIE
there. For example, if you look at the projects developed
with the waterfall life cycle, maybe you will find larger
DIE than for the other life cycles, which might partially
justify the claims of previous researchers about the
existence of DIE. This would also show that newer kinds of
life cycles (like Agile ones) help solve DIE. You do provide
hints about this in Section 8 ("We also note that other
development practices have changed in ways that could
mitigate the delayed issued effect") and Section 9, but
that's clearly not enough.
\begin{quote}{\em
If your comment is along the lines of ``look for stratifications to check if
DIE exists in subsets'' then we offer the following failed study. 

We looked into those stratifications and have  failed to find a stratification
containing an exponential expansion in the time to fix issues. The reason for this was due to the data in  those
stratifications resulted in smaller sample sizes that exacerbated the large IQR's seen in this data
(to see these large IQRs,  please consider the 50th and IQR columns of Figure~\ref{fig:fix}.IQR is the
range 75th to 25th in the data. Note that most of our IQRs were larger than tan the 50th percentile; i.e. software
data exhibits large variances-- where are exacerbated by the smaller samples seen in the stratifications.}
\end{quote}
Section 8

 
- [ ] Verify that the Section #'s in our responses are accurate prior to submission.

