The section presents the data that we use to conclude that phase delay is a not a major
contributor to the time required to resolve an issue.  

The Software Engineering Institute (SEI) at
Carnegie Mellon University explores methods for software process improvement.
One product from that research is  
Watts Humphrey's  Team Software ProcessSM (TSPSM)~\cite{tsp00}. TSP is an extension of Humphrey's early
work on Personal Software
ProcessSM (PSPSM)~\cite{psp05}.


Since 2004, one  of use (Nichols) has been been mentoring 171 software development teams around the world
as they  deploy TSP within their organizations.  
The projects
were mostly small to medium, with a median duration of 46 days
and a maximum duration of 90 days. Median team size was 7
people, with a maximum of 40. Hence,  total development 
effort\footnote{
Effort =  duration*teamSize/workDaysPerYear.} for
these projects ranged from 1.3 years (median) to 14.3 years (max).
The majority of
the projects were either web portals or banking systems in the US, South Africa, and Mexico. 
There were also some  medical devices (US, France, Japan, and Germany),  a few from a commercial 
computer-aided design system (developed by a world wide distributed team), and less that  
a dozen United States
Department of Defence projects.
 An anonymized version of that data is available in the PROMISE repository\footnote{\carter{Need to add}} and,
 for confidentiality restrictions, we cannot offer 
further details
on those project.  

\bill{can we offer any more deatils here?}

To aid in monitoring these 171 projects  Nichols  built a
data collection tool that gathers project data a central database at SEI. 
The issue types collected in the the SEI TSP data set divide into:
\be 
\item Environment: design, compile, test, or other support system problems;
\item Interface: procedure calls and reference, I/O, user format;
\item Data: structure, content; 
\item Documentation: comments, messages;
\item Syntax: spelling, punctuation typos, instruction formats
\item Function: logic, pointers, loops, recursion, computation, function defects  
\item Checking: error messages, inadequate checks;
\item Build: change management, library, version control;
\item Assignment: package
declaration, duplicate names, scope, limits;
\item System: configuration, timing, memory.
\ee
The tool also collected process information about those defects.
That data includes  work start time, work end time, delta
work time, and interruption time. Software engineers are often
interrupted by meetings, requests for technical help, reporting, and
so forth. These events are recorded, in minutes, as interruption
time. In this paper, when we report ``time to resolve an
issue'', we show the difference between the start and end times
of a work session, with any interruption time subtracted (the
difference in times, minus the interruptions).  

Three common features of all those  are (1)~coaches,
(2)~peer review, and  (3)~personnel reviews.
 
A ``coach'' is a team member  authorized to submit project data
Before submission,
these coaches check the data for obvious errors.

``Personnel review'' is a technique taken  from the Personal Software
ProcessSM (PSPSM). PSPSM encourages developers to continually make and review their personnel estimates
about their day-to-day tasks, then compare those estimates against the actual development effort.
In this way, developers can acquire a more realistic understanding of their work behaviour.

 
``Peer review'' is a  technique in
traditional software engineering.
 Basili and Boehm write  commented in 2001~\cite{boehm01} 
that peer reviews can catch over half the defects introduced into a system.
Peer review can be conducted on any artifact generated anywhere in the software
lifecycle and can quickly be adapted to new kinds of artifacts.



As of January 2014, the SEI TSP database contained data from 109
TSP projects. The projects started between July 2009 and
September 2013; they included 34 teams and 309 people. Among
the database fact tables, the time store contains 103,023 time logs,
18,408 defect logs, and 7,464 size logs\footnote{\bill{we need to 
update this table}}.  


%That said, certain semantic features of the SEI TSP data should be noted.
%Firstly, in the current TSP collection tool, 
%fix times are only the developer time for the developer walking through the phases of \fig{waterfall}.
%We are currently tracking the fix time for post-release issues (e.g. those raised during  acceptance test and %later
%product life cycle). So far, in that post-release data,  we have not detected
%a dramatic phase escalation effect (but at this time, we have nothing definitive comment on that matter).%
%
%\bill{somewhere you have one note on \underline{find} and fix times.  for this paper, we need just fix times. %but is there
%anything we need to fret about re \underline{find} times?}
 
\subsection{Project Phases}
This paper studies the impact of phase delay on the time required to resolve issues.
The phased used in this paper are shown in \fig{waterfall}. Note that, in that figure:
\bi 
\item
Several  phases have the same  sub-activities of {\em review} and {\em inspect}\footnote{\bill{plz distinguish review and inspect. does one usually happen first?}}
\item Testing is divided into several statges \footnote{\bill{need a one
line description of test vs qualtest vs inttest vs systemTest vs AcceptTest. I tries some words in the Key to \fig{sedata}. dont know if i got it right.}}
\ei 
Also shown in \fig{waterfall} (bottom left) is the standard definition of an agile process~\cite{boehmturner03}:
Given some
some backlog of tasks, wgeb teams complete their current tasks, they select the next task(s) to complete. That selection process may use a variety of criteria
to prioritize which  tasks are selected (for more details on that selection process, see~\cite{me09j,port08,boehmturner03}). Tasks are completed in ``sprints'' that can last hours,
days, but rarely not more than weeks. Each day meet for brief ``scrum'' sessions to assess (and possibly alter) their current progress on the goals of the sprint.  
Agile teams race to generate releases
(in the continuous release model, releases can be generated on a daily basis, or even faster).  
Experience gained from those releases informs the discussion in the daily scrums which, in turn,
can inform the team's decisions on how to select and implement the next set of tasks for next sprint.
In fact, that experience can result in changing some/all of the tasks in the current backlog. 

TSP is not antithetical to agile--  indeed a TSP-waterfall style project can adopt aspects
of agile.  For example the agile loop  could be applied
over one or more of the phases shown in the long waterfall chain of \fig{waterfall}. Some
TSP teams adopt something like test-driven-development\footnote{TDD is an agile-method
where the ``test'' is the primary driver of the design. The tests are written first,
then the code to support those tests. TDD proceeds in three steps: red (where there
are broken tests); green (where tests are passing); and, possibly, refactor (where
the code is re-organized based on feedback from those tests~\cite{fraser03}.} where
reviews are scheduled after testing. Some of the groups in the SEI TSP data used that approach but
they do not effect the main conclusions of this paper:
\bi 
\item They were strongly in the minority\bill{any numbers on this?};
\item Most of our phase delay data comes from much early in the waterfall model
of \fig{waterfall}.
\ei 
That said, there are some very ``un-agile'' aspects
of the processes used by the   projects in the TSP SEI data.
A TSP participant spends much time reflecting on the project,
undisturbed by the behaviour of the  executables.
In fact,
it can be days/weeks
before TSP participants gain  feedback from executing code, for the following reasons:  
\bi 
\item
For TSP projects, the times spent in the of a design activity/phase is   
approximately as much effort as the coding phase.  
\item Our TSP teams did not
 combine development and  testing. 
 \item  The TSP projects studied here offer new releases at least every three months, but
often much longer that that\bill{any numbers on that}. Note that this is a far slower
release schedule than seen in, say, continuous integration projects.  
\item The time devoted to personnel review'' and peer review (defined above)
is  about 50\% as much effort as the previous construction phase activity.
 
\ei

Having presented all that, we can now present the main point of this section:
\bi 
\item The results section of this paper fails to find that  phase delay causing dramatic increases
in time to fix issues;
\item This lack-of-phase-delay effect {\em cannot} be explained away just by  saying that
the projects in this sample adopted something like  agile methods to reduce re-work costs.
\ei  