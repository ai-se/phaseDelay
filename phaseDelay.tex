\documentclass{sig-alternate}
\usepackage{multirow}
\usepackage[usenames, dvipsnames]{color}
\usepackage{colortbl}


\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{picture}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}} 
\newcommand{\lucas}[1]{\textcolor{red}{LUCAS: #1}} 
\newcommand{\tim}[1]{\textcolor{Red}{TIM: #1}}
\newcommand{\bill}[1]{\textcolor{blue}{BILL: #1}} 
\newcommand{\carter}[1]{\textcolor{cyan}{CARTER: #1}} 
\newcommand{\sei}[1]{\textcolor{RedViolet}{BILL/FORREST: #1}} 
\newcommand{\todo}[1]{\textcolor{Maroon}{TODO: #1}} 
%\newenvironment{changed}{\par}{\par}

%timm tricks
\newcommand{\bi}{\begin{itemize}}%[leftmargin=0.4cm]}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
 

\usepackage[shortlabels]{enumitem} 
%\usepackage{times}

\usepackage{cite}
\newcommand{\subparagraph}{}
\usepackage{url}
\def\baselinestretch{1}


%\setlist{nosep}

\usepackage{colortbl}
 %\usepackage[font={small}]{caption, subfig}
%\setlength{\abovecaptionskip}{1ex}
 %\setlength{\belowcaptionskip}{1ex}
% \setlength{\floatsep}{1ex}
% \setlength{\textfloatsep}{1ex}
%\usepackage[compact,small]{titlesec}
\DeclareMathSizes{7}{7}{7}{7} 
\pagenumbering{arabic}
%\setlength{\columnsep}{7mm}

\begin{document}

\conferenceinfo{ICSE}{'16 Austin, Texas}
\title{Are Older Bugs More Expensive to Fix that Newer Bugs? }
\numberofauthors{3}
\author{
%\alignauthor 
Tim Menzies \\
       \affaddr{Computer Science}\\
       \affaddr{NC State University, USA}\\
       \email{tim.menzies@gmail.com}
\and%alignauthor 
William Nichols, Forest Shull \\
        \affaddr{Software Engineering Institute}\\
        \affaddr{Carnegie Mellon U, USA}\\
        \email{\{wrn,fjshull\}@sei.cmu.edu} 
\and %\alignauthor 
Lucas Layman \\
       \affaddr{Fraunhofer CESE} \\
       \affaddr{College Park, USA}\\ 
       \email{llayman@fc-md.umd.edu}
} 
\maketitle
\begin{abstract}
Does
repair time increase dramatically
the longer an issue persists in a system?
Many industrial practitioners and academics this   to be true.
This   belief is used to justify 
major investments in  software development
 processes and methods.

This
paper reports on the largest study on phase delay yet published.
Based on a sample of 
171 software projects from around the world (2006 and 2014),
 we find that this belief is  
incorrect. Specifically: the    time  to resolve an
issue   does {\em not} increase
the longer that issue remains in the system.    

This result raises the question: how do widely-held beliefs like phase delay survive so long  without  critical reassessment?  As a community, we need to consider how (and when) we reflect on our prominent hypotheses.
\end{abstract}

% A category with the (minimum) three required fields
\vspace{1mm}
\noindent
{\bf Categories/Subject Descriptors:} 
D.2.8 [Software Engineering]: Process metrics.

 

\vspace{1mm}
\noindent
{\bf Keywords:} software economics, phase delay, cost to fix.

\section{todo}

four hypotheses: time = cost;
intra project PD;
post project PD;
intra predicts for post

comment on why post is no longer current. continuous deployment. package managers.Windows moving from service patches (which occur rarely) to continuous deployment (so there will be no windows 11... just a stream of continuous updates).   architectural incubators: big superstructures within which it is simpler and cheaper to do many small projects. e.g. aircraft carriers,  packet-stitching. mean stack (to be clear: none of the data explored in this paper comes from such architectural incubators: this paragraph is only meant to be a  speculation of future implications of this old data to newer projects). 

\section{Introduction}
In 2013-2014, 
eleven  million programmers~\cite{pettey14} and
half a trillion dollars~\cite{avram14} were spent on information technology.
Such a large and growing effort should be managed and optimized via  well-researched conclusions.  

It is standard practice
in other fields such as medicine,
to continually revisit old conclusions~\cite{prasad13}.
Accordingly, this paper revisits
the {\em phase delay effect}
i.e.,  the rule of thumb stating that the time required to resolve an issue in a software project increases dramatically 
the longer it remains in the system. Basili \& Boehm comment that since the 1980s,  phase delay
``...has been a major driver in focusing
industrial software practice on thorough
requirements analysis and design,
on early verification and validation, and
on up-front prototyping and simulation
to avoid costly downstream fixes''~\cite{boehm01}.

Basili and Boehm are not alone in this assessment of the significance of the phase delay effect.
For example,
our surveys of industrial practitioners (shown in \S\ref{survey}) reveal the phase delay effect to be
one of the most strongly-held beliefs amongst commercial software engineers.
Also,  previously~\cite{me08a} we used phase delay to justify the software practice
of 
Independent Verification \& Validation (IVV). Specifically,  when the IVV team finds bugs earlier than the developers then (due to the phase delay effect) this provides overall cost savings to the project. 
Note that this argument critically depends on the assumption that fixing later-phase issues is  significantly more expensive than fixing them earlier (as does the views of Boehm and Basili and all the industrial practitioners
we interviewed in \S\ref{survey}). 

Accordingly, in this paper, we investigate several hypotheses related to this widely-held belief:
\bi
    \item $H_1$: The cost to resolve issues dramatically increases across phases.
    \item $H_2$: The cost to resolve issues dramatically increases as the phase delay increases.
\ei
As we explored the above, we found a related hypothesis was believed by both professional software engineers and some academics:
\bi
 \item $H_3$: Requirements errors are the most expensive to fix.
\ei
The rest of the paper investigates these  hypotheses. First, we conduct a survey of practitioners and researchers to gauge their belief in the phase delay effect. Second, we  analyze 171 software  projects developed in the period 2006 to 2014.  

% The conclusions of this paper is that it is erroneous to believe in phase delay.
% When we looked for supporting evidence for that effect, we found that:
% \be 
% \item The evidence for that effect in the historical literature is very weak (to say the least);
% \item  In  a large sample of   contemporary software projects, there is no evidence for phase delay.
%  \ee
% The rest of this paper present evidence for the above two points. 
% The first point is  documented via  a literature survey and the second point is demonstrated
% via an analysis of  

The conclusions of this paper are two fold. First, the phase delay effect does not hold in the large body of software projects analyzed. Second, commonly-held beliefs, such as phase delay, must be periodically revisited empirically in order for researchers to provide accurate and up-to-date guidance for the practitioner community.

% the SE community needs to stop
% justifying changes to SE
% practices via the phase delay effect. Secondly,   myths like phase delay should not have lasted decades
% in the literature without  critical reassessment.  As a community, we need to reflect more on our
% research practices and on how (and when) we reflect and revisit the prominent hypotheses of our field.

The core of this paper's arguments are two figures: Figure~\ref{fig:fix-phase-dist} and  \fig{raw}.
Lest the reader find this too little  to justify our conclusions, we note that it took months
of work to distill this data from our 171 projects, after which the five authors spent weeks
exploring and debating the nuances. To the best of our knowledge, this is the largest study on phase delay yet published (and it contains data from orders of magnitude
more projects than used in prior work~\cite{Fagan76,Boehm76,Daly77,Stephenson76,Shull02,Royce98,Clutterbuck09,Elssamadisy02}).
To ensure reproducibility and unlike previous work,
all the data  used in this study is available in the PROMISE
repository (openscience.us/repo).

\section{Motivation}
\subsection{Why Reassess Existing Truisms?}

Before exploring the commonly-held belief of the {\em phase delay effect}, we digress to discuss the merits of revisiting
old conclusions in software engineering.


Belief in general
principles of software
engineering are common to both research and practice. Professional societies assume such generalities exist when they offer
 lists of supposedly general ``best practices'' such as
the IEEE 1012 standard for software verification~\cite{1012}.  Endres \& Rombach's comment that 
\begin{quote}
Based on repeated and consistent observations, key lessons of these fields can now be formulated into rules or even laws, providing initial building blocks towards a theoretical foundation that is essential for further research, for teaching and for the practice of software development~\cite{endres03,rombach11}
\end{quote}
 Endres \& Rombach offer dozens of lessons of software engineering~\cite{rombach11}.
 Many other 
widely-cited researchers  do the same; e.g.
Glass~\cite{glass02}; Jones~\cite{jones07}; Boehm~\cite{boehm00b}.
Budgen \& Kitchenham seek to reorganize SE research using
general
conclusions drawn from a larger number of studies~\cite{kitch04,budgen09}.
Nevertheless, there are many empirical  findings that 
cast doubt on the existence of  ``key lessons  that can be formulated
as general laws of SE'':
\bi
\item
Turhan~\cite{me12d} lists 28 studies with contradictory conclusions
on the relation of OO measures to defects.  Those results
 directly  contradict some of the laws listed by 
Endres \& Rombach~\cite{endres03}.

\item
Ray et al.~\cite{ray2014lang} tested if   strongly typed languages
predict for better code quality. In  728 projects,
they found  only a modest benefit in strong typing
(and caution that that effect may actually be due to other  conflating factors).
\item
Fenton \& Neil~\cite{fenton00,fenton00b}   critique the truism that
``pre-release fault rates for software
are a predictor for post-release failures'' (as claimed by~\cite{dunsmore88},
amongst others). For the systems described in~\cite{fenton97}, they
show that software modules that were highly fault-prone
prior to release revealed very few faults after release.
\item
Meyer claims that   object-oriented (OO) encapsulation will
reduce error rates in software~\cite{Meyer1988}.  Yet empirical results suggest
that debugging an OO program is many times harder and
longer than debugging a standard procedural program~\cite{hatton98}.
\item A truism of visual programming is that ``visual
representations are inherently superior to mere textual representations''. A review by Menzies suggests that the available
evidence for this claim is hardly conclusive~\cite{me00v}. 
\item Numerous recent {\em local learning} results compare single models
learned from all available data to multiple models learned from clusters within the data~\cite{betten14,yang11,yang13,minku13,me12d,me11m,betta12,posnett11}.
A repeated result in those studies in that the local models generated the better effort
and defect predictions (better median results,
lower variance in the predictions).
% \item 
% Other work studies  found dozens of single
% models that performed worse that {\em committees of models}, all with % different opinions,
% that contribute to an {\em ensemble} prediction of software 
% development effort~\cite{kocaguneli2012value,azhar13}.
\ei
%To be fair to the software engineering community,
Software engineering is not the only
field where practitioners hang on to persistent beliefs, even if the evidence
for those beliefs is not strong.
The medical profession applies  many practices based on studies
that have been disproved. For example,
a  recent article
in the Mayo Clinic Proceedings~\cite{prasad13} found  
146 medical practices based on studies 
in year $i$, but which were  reversed by subsequent trials within years $i+10$.
Even when the evidence for or against a treatment or intervention is clear, medical providers and patients may not accept it~\cite{aschwanden10}.
Aschwanden warns:
\begin{quote}
Cognitive biases - such as motivated reasoning (all of us want to believe that the things we do make a difference), base rate neglect (failing to pay attention to what happens in the absence of the intervention), and confirmation bias (the tendency to look for evidence that supports what you already know and to ignore the rest) - also influence how we process information~\cite{aschwanden15}.
\end{quote}

The cognitive issues that complicate medicine are also found in software engineering.
According to Passos et al.~\cite{passos11}, commercial developers
are all too willing to believe in general
truisms.
They  warn that developers
usually assume that the truisms they learn from a few past
projects are general to 
all their future projects:
\begin{quote}\label{q:pass}
...the past experiences were taken into account without 
much consideration for their context~\cite{passos11}.  
\end{quote}
The results of J{\o}rgensen \& Gruschke~\cite{jorgensen09} concur with Passos et al. They report that 
  supposed software engineering    ``experts'' rarely use lessons
  from past projects to improve their future reasoning~\cite{jorgensen09}. 
 They note that
when the experts
  fail to revise their beliefs, this leads to poor
 conclusions and software projects  (see examples in~\cite{jorgensen09}).
%% This leads to  detrimental
%% situations where groups or 
%% individuals working on the same project have conflicting beliefs, which they never debate
%or refine or unify.

In summary, just like medicine, our field suffers when
 software engineers do  not revise old beliefs.  Therefore, it is important
 to regularly  reexamine    old beliefs.
 
\subsection{Why Study Phase Delay?}
\label{sec:why-study}
%\todo{\begin{itemize}
%\item Given the Intro and Motivation, this section should focus on the influence of phase delay in software engineering milieux
%\item Need to tone down the reliance on Beck. The important Beck point is that eliminating the cost-to-fix curve (resulting from phase delay) is one of the main goals of agile. His observations on new technologies are good too.
%\item Need to cite other popular sources of this information, e.g., McConnell's "Ounce of Prevention" article~\cite{mcconnell01}.
%\item Robert Glass's~\cite{glass02} book is very popular and contains an entry on this point
%\item Google Scholar Citations for phase delay works Boehm88~\cite{boehm88} (673), Boehm \& Basili~\cite{boehm01} (761), 
%\end{itemize}}

This paper examines evidence for the phase delay effect due to its prominence throughout the field of software engineering. Many prominent authors have commented on its perceived usefulness as a rule of thumb for software engineers. For example, in 2001, Steve McConnell mentioned it as a "common observation" in the field and proceeded to knock down arguments that it no longer held for contemporaneous software projects, by relying on recently published data and experience. McConnell also summarized the intuitive argument for why it should be so: ``A small mistake in upstream work can affect large amounts of downstream work. A change to a single sentence in a requirements specification can imply changes in hundreds of lines of code spread across numerous classes or modules, dozens of test cases, and numerous pages of end-user documentation''~\cite{mcconnell01} Glass echoes this sentiment, stating the the fact ``requirements errors are the most expensive to fix when found during production but the cheapest to fix early in development'' is ``really just common sense''~\cite{glass02}. 

Also in 2001, the phase delay effect was listed as number one by Boehm and Basili in their "Top 10 list" of "objective and quantitative data, relationships,
and predictive models that help
software developers avoid predictable pitfalls
and improve their ability to predict
and control efficient software projects." \cite{boehm01}

More important than its use as a rule of thumb, the phase delay effect has 
been used for decades to motivate major changes to software engineering practices (as mentioned in Section 1). For example,
in 2000, Kent Beck used phase delay to motivate his call for ``extreme programming'' (a set of techniques
that evolved into contemporary agile and DevOps practices). Beck writes:
\begin{quote}
One of the universal assumptions of software engineering is that the cost of changing a program rises exponentially over time. I can remember sitting in a big linoleum-floored classroom as a college junior and seeing the professor draw on the board the curve found in \fig{curve1}... 
%
%The cost to fix a problem in a piece of software rises exponentially over time. 
A problem that might take a dollar to fix if you found it during requirements analysis might costs thousands to fix once the software is in production.
%
%I resolved then and there that I would never let a problem get through to production. No sirree, I was going to catch problems as soon as possible. I would work out every possible problem in advance. I would review and crosscheck my code. No way was I going to cost my employer \$100,000.~
\cite{beck00}
\end{quote}

\begin{figure}
 \includegraphics[width=3.3in]{img/beckB4.png}
 \caption{Projects exhibiting phase delay. From~\cite{beck00}.}\label{fig:curve1}
 \end{figure}
\begin{figure}
 \includegraphics[width=3.3in]{img/beckAFTER.png}
 \caption{Projects without   phase delay. From~\cite{beck00}.}\label{fig:curve2}
\end{figure}

% \begin{figure}[!t]
% \scriptsize
% \begin{center}
% \begin{tabular}{l@{~}|l@{~}|r@{~}r@{~}|r@{~}l}
%           &        & \multicolumn{2}{c|}{Time to}  &\\
%           &        & \multicolumn{2}{c|}{resolve} &\\
% Injection & Removal& initial & now & \multicolumn{2}{l}{Scale up w.r.t initial} \\\hline
% Phase $i$&Phase $i$ & 1 & 1 & 1 & **\\
% Phase $i$ &Phase $i+1$ & 1 & 2& 2 & **** \\
% Phase $i$ &Phase $i+2$ & 1 & 4& 4 &******** \\
% Phase $i$ &Phase $i+3$ & 1 & 8 & 8 & ****************  
%   \end{tabular}\end{center}
%  \caption{Projects data consistent with
%  the phase delay effect of \fig{curve1}.}\label{fig:curve1a}
% \end{figure}

% \begin{figure}[!t]
% \scriptsize
% \begin{center}
% \begin{tabular}{l@{~}|l@{~}|r@{~}r@{~}|r@{~}l}
%           &        & \multicolumn{2}{c|}{Time to}  &\\
%           &        & \multicolumn{2}{c|}{resolve} &\\
% Injection & Removal& initial & now & \multicolumn{2}{l}{Scale up w.r.t initial} \\\hline
% Phase $i$&Phase $i$ & 1 & 1 & 1 & ** \\
% Phase $i$ &Phase $i+1$ & 1 & 2& 2 & **** \\
% Phase $i$ &Phase $i+2$ & 1 & 2& 2 & **** \\
% Phase $i$ &Phase $i+3$ & 1 & 2 & 2 & **** \\
%  Phase $i$  &Phase $i+4$ &1 & 2& 2 &**** 
%   \end{tabular}\end{center}
%  \caption{Projects data consistent with
%  the phase delay effect of \fig{curve2}.}\label{fig:curve2a}
% \end{figure}
 
Beck speculated that there might be another kind of project, shown in \fig{curve2},
that does not exhibit phase delay. In fact, for Beck, the possibility that phase delay does not imply exponential increases in the resources required to make changes and fix defects is {\em the} major
motivation for his work, because it means that decisions can be made as late in the process as possible, so as to increase the chances that these decisions would be right. Beck found more than adequate motivation for this new assumption:
%\begin{quote}
%This (\fig{curve2}) is one of the premises of (agile). It is the technical premise of (agile). If the cost of change rose slowly over time, you would act completely differently from how you do under the assumption that costs rise exponentially. You would make big decisions as late in the process as possible, to defer the cost of making the decisions and to have the greatest possible chance that they would be right. You would only implement what you had to, in hopes that the needs you anticipate for tomorrow wouldn't come true. You would introduce elements to the design only as they simplified existing code or made writing the next bit of code simpler.~\cite{beck00}
%\end{quote}
%Beck makes one other comment that is relevant to this paper: 
\begin{quote}
The software development community has spent enormous resources in recent decades trying to reduce the cost of change-better languages, better database technology, better programming practices, better environments and tools, new notations.

What would we do if all that investment paid off? What if all that work on languages and databases and whatnot actually got somewhere? What if the cost of change didn't rise exponentially over time?~\cite{beck00}
\end{quote}
The results of this paper are that, in our sample of hundreds of projects, there is no evidence for phase delay.
This result is   consistent with Beck being correct: all that work on    languages and databases
have paid off and, hence, modern software projects do not suffer from the  phase delay effect.  

That said,  the phase delay myth persists. Hence, this paper.

%XXX clumsy link here
%That said, where our results
%diverge from Beck is that we have observed this lack of phase delay in  projects that are not agile. Those
%projects are described below.
 

\subsection{Survey:  Phase Delay and Cost-to-fix Belief}\label{survey} 
Yet another reason to study the phase delay effect is that it is a dominant belief in industrial SE.
To test whether this was indeed true, we conducted a survey of software engineers. If our surveyed practitioners make management decisions based on their
understanding of SE theory, then the phase delay effect may well influence their judgment.

\input{lucas}

\section{Related Work}
 

\input{lucasRelatedWork}







%\section{carterCharts}

%\input{carter}

 

\section{Data Collection}


\input{data}







\section{Analysis of Phase Delay in 171 TSP Projects}
Our units of analysis are:
\bi
    \item \emph{defects} - individual defects are recorded as line items in the defect logs uploaded to the SEMPR at the SEI. One or more defects are reported against a single \emph{plan item} in the time tracking logs, e.g., a review session, an inspection meeting, a test execution.
    \item \emph{time} - Time is tracked per person per plan item in the time-tracking logs, e.g. a 30 minute design review session involving 3 people will have three time log entries summing to 90 minutes. Time includes the time to analyze, repair, and validate a defect fix.
    \item \emph{time per defect} - The total \# of defects found in a plan item during a removal phase divided by the total time spent on that plan item in that phase.
\ei


%\todo{
%\bi
%    \item What is the cost-to-fix curve in TSP data? Check against H1 and compare to past literature.
%    \item What is the effect of phase delay? Check against H2.
%    \item \fig{cost-to-fix-tsp} is great. really important. its from  different decades, right?  so can we add text saying that the general pattern is that the (approx) 15 years between each curve tends to a flattening. not, perhaps, due to process but to better build environs.
%\ei
%}

 
 

\begin{figure}[!t] 

%\hspace{-0.7cm}
\begin{center}
\includegraphics[height=2in]{img/fix-phase-dist.pdf}
\end{center}
%~\\

\caption{Distribution of defects found and fixed by phase.}
\label{fig:fix-phase-dist}
\end{figure}



\begin{figure}[!t] 
%\vspace{-1cm}
%\hspace{-0.35in}
\begin{center}
\includegraphics[width=3in]{img/boehm-overlay.pdf}
 \end{center}
% ~\\~\\
%~\\~\\~\\
 
 \caption{Cost-to-fix curve from \protect\fig{cost-to-fix} 
 overlayed with case study from~\cite{Royce98} and TSP data. Scale is relative to the phase with the lowest cost to fix. }\label{fig:cost-to-fix-tsp}
 \end{figure}

 
\begin{figure*}[!t] 
 \renewcommand{\baselinestretch}{0.7}
 \scriptsize
\begin{center}
\begin{tabular}{r|rrr|ll|rl}  % I attach your cursor. thrust, parry, yo!
  Sample&\multicolumn{3}{c|}{Percentiles}\\ 
size & 25th & 50th & 75th & Phase injected & Phase removed & \multicolumn{2}{l}{Scale up w.r.t. to first phase}\\\hline
\\
 48&   14&   30&   51&Before&DesignInspect &1.00 & **********  \\
 53&   13&   28&   49&Before&CodeReview &0.93 & **********  \\
 94&   22&   45&   80&Before&CodeInspect &1.50 & ***************  \\
153&   38&   77&  130&Before&UnitTest &2.56 & **************************  \\
127&   30&   65&  128&Before&IntTest &2.16 & **********************  \\
 92&   24&   56&   89&Before&SysTest &1.86 & *******************  \\
 
 
\\
 56&   14&   35&   57&Planning&ReqtsReview &1.00 & **********  \\
 46&   11&   24&   39&Planning&DesignInspect &0.68 & *******  \\
 32&   18&   33&   84&Planning&UnitTest &0.94 & **********  \\
 
\\
184&   45&   92&  155&Reqts&ReqtsReview &1.00 & **********  \\
189&   46&   93&  155&Reqts&ReqtsInspect &1.00 & **********  \\
 61&   14&   33&   55&Reqts&DesignReview &0.35 & ****  \\
102&   24&   49&   83&Reqts&DesignInspect &0.52 & ******  \\
 59&   18&   38&   60&Reqts&CodeInspect &0.40 & ****  \\
 91&   22&   49&  104&Reqts&UnitTest &0.52 & ******  \\
100&   33&   67&  120&Reqts&IntTest &0.72 & ********  \\
 78&   28&  122&  237&Reqts&SysTest &1.31 & **************  \\
\\
210&   51&  103&  166&Design&DesignReview &1.01 & ***********  \\
207&   50&  101&  156&Design&DesignInspect &1.00 & **********  \\
134&   32&   66&  116&Design&CodeReview &0.65 & *******  \\
163&   39&   79&  135&Design&CodeInspect &0.78 & ********  \\
238&   58&  119&  183&Design&UnitTest &1.17 & ************  \\
172&   42&   88&  158&Design&IntTest &0.87 & *********  \\
163&   40&   80&  161&Design&SysTest &0.79 & ********  \\
 80&   21&   45&   73&Design&AcceptTest &0.44 & *****  \\
 
\\
235&   57&  115&  177&Code&CodeReview &1.00 &  **********  \\
228&   56&  113&  176&Code&CodeInspect &0.99 & ********** \\
243&   59&  125&  206&Code&IntTest &1.08 &     *********** \\
191&   46&   95&  173&Code&SysTest &0.82 &     ******** \\
269&   66&  133&  206&Code&UnitTest &1.15 &    ***********  \\
130&   31&   65&  122&Code&AcceptTest &0.56 &  ******  \\
 \end{tabular}
\end{center}
\caption{Distribution of fix times seen in SEI TSP data. Left hand side bars
show scale up on median values.}
\label{fig:raw}
\end{figure*}




\subsection{TSP Project Cost-to-fix curve}
This section discusses our first hypothesis ($H_1$) that   that  the cost-to-resolve issues dramatically increases across phases.

The distribution of defects found and fixed per phase in our data is shown in Figure~\ref{fig:fix-phase-dist}. A high percentage of defects (44\%) were found and fixed in the early phases, i.e., requirements, high level design, and design reviews and inspections. (This distribution is similar to that observed for other projects that emphasized investment in software engineering quality assurance practices. For example, Jones and Bonsignour report 52\% of pretest defects removed before entering implementation, for large projects that focus on upfront defect removal techniques \cite{jones12}. NASA robotics projects had a slightly higher percentage (58\%) of defects removed before implementation began, although these had invested in IV\&V on top of other forms of defect removal \cite{me08a}.)  

The first thing to note in Figure~\ref{fig:fix-phase-dist}, 
is that this data does not follow the exponential increase feared by Beck's \fig{curve1}:
\bi
\item There is much ``up and down'' in the chart. 
\item If we separate out just the {\em Inspect} or {\em Review} data, we can see that there is some increase in {\em Design} and {\em Code}. However, that effect could be a linear or be just noise. In either case,
it does not look like \fig{curve1}.
\item The time in final testing is particularly low suggesting that few defects survived into testing
(and hence, there were few instances where code required massive rework).
\ei

For illustrative purposes, the relative cost to fix errors per lifecycle phase in the TSP project data is overlayed with Boehm's cost curve~\cite{Boehm81} as well as the CCPDS-R case study~\cite{Royce98} in \fig{cost-to-fix-tsp}. Cost-to-fix for TSP is total time spent in each review, inspection, or test-related phase / \# of defects found in phase (reported in \fig{raw}). It is clear to see that the trend is relatively flat, closer to the pattern exhibited by the Royce case study (which also focused on early defect removal) than the exponential growth curve.

%\begin{table}[ht]
%\renewcommand{\baselinestretch}{0.7}
% \scriptsize
%\centering
%\begin{tabular}{ll|ll}
%  Phase num & Name & \# defects & percentage \\ 
%  \hline
%1 & BeforeDevelopment &   0 & 0.00 \\ 
%  2 & Planning &   4 & 0.00 \\ 
%  3 & Reqts &  25 & 0.10 \\ 
%  4 & ReqtsReview & 2304 & 6.50 \\ 
%  5 & ReqtsInspect & 2630 & 7.40 \\ 
%  6 & HLD &   0 & 0.00 \\ 
%  7 & HLDReview & 790 & 2.20 \\ 
%  8 & HLDInspect & 818 & 2.30 \\ 
%  9 & Design &  94 & 0.30 \\ 
%  10 & DesignReview & 3668 & 10.30 \\ 
%  11 & DesignInspect & 5365 & 15.00 \\ 
%  12 & Code & 833 & 2.30 \\ 
%  13 & CodeReview & 6276 & 17.60 \\ 
%  14 & CodeInspect & 6623 & 18.60 \\ 
%  15 & UnitTest & 4283 & 12.00 \\ 
%  16 & IntTest & 1024 & 2.90 \\ 
%  17 & SysTest & 730 & 2.00 \\ 
%  18 & AcceptTest & 189 & 0.50 \\ 
% \end{tabular}
%\caption{Defects found per phase} 
%\label{fig:defect_per_phase}
%\end{table}



%\begin{figure}[!ht]
%
% \renewcommand{\baselinestretch}{0.7}
% \scriptsize
%\begin{center}
%\begin{tabular}{r|rr|rl}
%Phase & samples & total time&\multicolumn{2}{l}{Total time w.r.t. first phase}\\\hline
%~\\
%Planning&229&12625&1.00&*\\
%~\\
%Reqts&590&24182&1.91&**\\
%ReqtsInspect&2533&92723&7.34&********\\
%ReqtsReview&1762&67471&5.34&******\\
%~\\
%HLD&78&6856&0.54&*\\
%HLDReview&472&12821&1.01&**\\
%HLDInspect&1366&35291&2.79&***\\
%~\\
%Design&620&21573&1.70&**\\
%DesignInspect&8373&229937&18.21&*******************\\
%DesignReview&3043&92823&7.35&********\\
%~\\
%Code&3029&220058&17.43&******************\\
%CodeReview&4774&163091&12.91&*************\\
%CodeInspect&11406&334442&26.49&***************************\\
%~\\
%UnitTest&9177&352245&2.790&****************************\\
%IntTest&2761&160559&12.71&*************\\
%SysTest&2059&105796&8.37&*********\\
%AcceptTest&1045&42717&0.338&****\\
%\end{tabular}
%\end{center}
%\caption{Times per phase (minutes).}
%\label{fig:fix-time-per-phase}
%\end{figure}




Given the above evidence, we reject $H_1$: there is no clear evidence that  cost-to-fix of defects dramatically increases across phases.

\subsection{Phase delay analysis} 
 
 \subsubsection{$H_2$: Cost-to-resolve Increases with Delay}

%we calculate phase delay as simply the number of project phases (the set used in our data set is shown in %\fig{waterfall}) during which the defect existed in the system, i.e. the ordinal rank of removal phase minus the %ordinal rank of injection phase. Sub-phases (such as review and inspection) are included in this calculation.
%
\fig{raw} shows the 25th, 50th, 75th percentile
of the time spent on {\em review} or {\em inspect} or {\em test} phases
in our 171 projects.
Note that, in TSP, when developers see issues, they enter {\em review} or 
{\em inspect} or {\em test}
until that issue is retired.

In that figure, the data is split out according to issues that were fixed in phase $Y$ after
being introduced in phase $X$. The data is sub-divided into tables according to $X$;
i.e. according to {\em before, planning, requirements, design} or {\em  code}. 

The stars on the right-hand-side of that figure show the scale ups associated with the phase removal stages,
expressed as a ratio of the first item in that block (which, by definition, is 1.00 for
that first item).
If $H_2$ was correct, we would expect those bars in each sub-division to exhibit a  shape
like \fig{curve1}. 

They do not. Hence, we reject $H_2$.


 \subsubsection{$H_3$: Requirements Repairs are Most Expensive}
 
As to $H_3$, the {\em requirements} the  median time spent fixing errors in {\em requirements} was 92,93  49,33,39,49,67,122 minutes\footnote{If this time seems long, then recall
that it includes the time required  to (a)~collect data and realize there is an error;
(b)~prepare a fix;  and (c)~apply some validation
procedure to check the fix (e.g. discuss it with a colleague or execute some tests).}.
This is:
\bi
\item
Clearly longer than median times seen {\em Before } and in {\em Planning}
(30,28,45,77,65,56,24,35,33);
\item
Somewhat shorter, but still not exponentially smaller,  than the median times see after {\em requirements} in {\em design}
(103,101, 79,66, 119,88, 80,45) and in {\em code} (65,115, 113,125, 95,133). 
\ei
For completeness, we ran an  
A12 test comparing the raw data (and not just the medians of \fig{raw}) and found
 a non-small effect only for the difference {\em requirements}
to {\em code}. But even though the effect was non-small, 
 the overall size of the difference
{\em requirements} to {\em code} was less than 30\% (i.e. much less than we would
expect from \fig{curve1}. 

Based on the above, we reject   $H_3$.

 
 %\begin{figure}[!t]
%\%renewcommand{\baselinestretch}{0.7}
%\scriptsize
%\begin{center}
%\begin{tabular}{l@{~~}|l@{~}|r@{~}|r@{~}r@{~}|r@{~}l}
%%           \multicolumn{2}{c}{~}                 &  &\multicolumn{2}{c|}{median}\\
 % Injection&   Removal& $n$ & initial & now & \multicolumn{2}{l}{Scale up, w.r.t. initial}
%\input{deltas}
%\end{tabular}
%%\end{center}
%\caption{50th percentile (median) scale ups  for  time to resolve issues (taken from \fig{raw}).}
%%\label{fig:scale}
%\end{figure}
 



 


%\subsection{Phase Delay in Slower Bugs?}
%In these results, we checked if a small number of bugs are most expensive. 
%To check this:
%\bi
%\item
%We repeated the analysis that generated \fig{scale},
%but instead of looking at the 50th percentile, we displayed the scale up factors
%\item 
%We only
%checked the Design and Code scale up results since, from \fig{scale}, it is clear these
%have the most examples of longest phase delays.
%\ei 
%The  results are shown in \fig{scale90} and these
%results are somewhat different for Design and Coding issues: 
%\bi 
%\item For Design issues,  these have the same
%general form as the 50th percentile results. That is, while it it certainly faster
%to remove thing sin the phase where they are created, once we leave that phase
%it does not seem to matter much how many phases we wait before fixing the issue.
%\item
%\item For Coding issues, the seems little impact of phase delay on the time
%required to fix.
%\ei

 


%\begin{figure}[!t]
%\renewcommand{\baselinestretch}{0.7}
%\scriptsize
%\begin{center}
%\begin{tabular}{l@{~~}|l@{~}|r@{~}|r@{~}r@{~}|r@{~}l}
%          \multicolumn{2}{c}{~}                 &  &\multicolumn{2}{c|}{99th }\\
%           \multicolumn{2}{c}{~}                 &  &\multicolumn{2}{c|}{percentile }\\
%  Injection&   Removal& $n$ & initial & now & \multicolumn{2}{l}{Scale up, w.r.t. initial}
%\input{deltasBigBig}
%\end{tabular}
%\end{center}
%%\caption{99th percentile scale ups.}
%\label{fig:scale99}
%\end{figure}

%\begin{figure}[!t]
%\renewcommand{\baselinestretch}{0.7}
%\scriptsize
%\begin{center}
%\begin{tabular}{l@{~~}|l@{~}|r@{~}|r@{~}r@{~}|r@{~}l}
 %          \multicolumn{2}{c}{~}                 &  &\multicolumn{2}{c|}{mean}\\
 % Injection&   Removal& $n$ & initial & now & \multicolumn{2}{l}{Scale up, w.r.t. initial}
%\input{deltasMean}
%\end{tabular}
%\end{center}
%\caption{Mean  scale ups.}
%\label{fig:scale90}
%\end{figure}


 


 

\section{Validity} 
We report validity concerns following the guidelines of Runeson \& H\"{o}st~\cite{runeson09}.

\subsection{Construct validity}
The essential constructs in our analyses are \emph{cost-to-fix, defects, and phase delay}. 

\emph{Cost-to-fix} is traditionally measured as either effort spent to fix a defect or time spent fixing a defect. In our analysis, we calculate the latter as \# of defects fixed in a defect-removal plan item, such as DesignReview or CodeInspection, divided by the time spent in the plan item. A single project has multiple such plan items. In the TSP data, this time accounts for time spent isolating, analyzing, and fixing the defect, but does not include time spent verifying the fix.

In our sample, all \emph{defects} are considered equally, and thus TSP's reporting methods may result in ``small'' defects could be dominating the sample. For example, typographical requirements errors are included as well as requirements errors that necessitate an architectural change to the system. Our analysis does not include a notion of defect severity, however, we observe no indications in previous cost-to-fix results reported by \cite{Boehm81} and \cite{Royce98} that severity was considered in their samples. However, evidence discussed in \cite{Shull02} suggests that low severity defects may exhibit a lower cost to change. Nonetheless, even ``small'' typographical errors have been known to cause enormous damage (e.g., the Mars climate Orbiter). 

\emph{Phase delay}, as depicted in the scale up column in \fig{cost-to-fix-tsp} and \fig{raw}, is a discrete value that does not consider the total time that a defect is latent in the system. Thus, the phase delay of two defects injected in Code but found in System Test will be the same even though one was found at the beginning of test while the other at the end of a potentially lengthy test period. Ideally, one could examine the total real time a defect was in a system from the point at which is was injected to the point at which it was discovered. It may be possible to gather such information if the project keeps rigorous change logs for its requirements, design, and code artifacts. 

\subsection{Internal validity}
Confounding variables are a threat to any study. In this research, we rely on the substantial sample size of 171 TSP projects and over 47,000 defect logs to avoid confounding effects that may arise from the nature of the projects, teams, or defects. Similarly, the 10-year data collection period should help to ameliorate any maturation or history effects.  As described in \S\ref{sec:data-collection}, all TSP teams are required to contribute time and defect data to the SEI, and thus there should be no selection bias in this sample compared to the overall population of TSP projects. However, there is likely selection bias in the teams that elect to use TSP compared to the entire population of software development teams. Further, we assume that each team has similar defect recording practices, and the TSP coaching provides guidance on what constitutes a defect. Nonetheless, individual developers and teams may apply their own internal rules for filtering defects, which would lead to inconsistent reporting thresholds among the projects in our sample.

\subsection{External validity}
Threats to external validity affect the generalization of findings. The most obvious threat to external validity is that TSP is not representative of most software development processes. TSP was, in fact, designed to be rigorous and eliminate defects early in the lifecycle and soon after injection. We make no assertions regarding the generalizability of these results beyond TSP, we observe that other case studies in various domains have exhibited flattened cost-to-fix curves.

Our analysis of cost-to-fix does not include defects found in production, which, in Boehm's original data, account for costs up to 200 times the cost of finding and fixing defects in Requirements. Further, it is not clear how Boehm~\cite{Boehm81} and Royce~\cite{Royce98} define \emph{cost-to-fix}. In our analysis, cost-to-fix includes the time spent analyzing, fixing the defect, and validating the fix. If Boehm and Royce do not include all of these costs, then the TSP cost curve in \fig{cost-to-fix-tsp} artificially high by comparison.

\subsection{Reliability}
A previous paper~\cite{shirai14} has applied a range of sanity checks to the data.
A common property of real-world data sets is the presence
of noisy entries (superfluous  or spurious data). 
The level of noise can be quite high. As reported
in \cite{shepperd12}, around
10\% to 30\%
of the records in the NASA MDP defect data sets are
affected by noise. Nichols et al.~\cite{shirai14}  report that
the noise levesl in the SEI TSP data are smaller than those seen
in other data sets. They found in the SEI TSP data that:\bi 
\item
4\% of the data was incorrect such as  null values of illegal formats;
\item  2\% of the data has inconsistencies such as timestamps
where the stop time was before the start time;
\item 3\% of the data had data that was not credible
such as tasks listed in one day that took like than six hours.
\ei 

%The definitions of defect, cost-to-fix, and phase delay are provided in earlier sections. The %source data and analysis scripts used to calculate the defect counts, cost-to-fix, and phase delay %from the raw materials can be found at \url{https://github.com/txt/txt.github.io/tree/master/tsp}. 
 

 
 
 \section{Conclusion}
 %\todo{
 %\bi
 %   \item Summarize data tested - survey + TSP data
%    \item reiterate hypothesis testing results
%    \item Restate the motivation: widely-held beliefs need to be revisited. 
%    \item We now have evidence, from 5 different sources, that phase delay does not seem to hold under certain conditions (although our study is by far the biggest in terms of data points).
%    \item phase delay may still apply in some (many?) projects, but clearly it does not in all. This has huge implications for, e.g., cost modeling, effort prediction.
% \ei
% }
 
In this paper, we have examined contemporary project data for evidence to support the well-known software engineering truism of the phase delay effect. 
We found that, in the literature, the evidence for phase delay was very spare.
%and (in a $21^{st}$).  %don't know what this is supposed to be
We suspect that a very old result was requoted, without being 
critically re-examined, for literally decades.

We  also  reported the results of a survey of both researchers and practitioners which showed this truism is still a widely held belief in the software engineering community. Such a widely
held belief, for something with so little evidence, can be damaging if ever it is used
to justify spending scarce resources on novel  methods and tools .

Using data from 171 projects from the period 2006 to 2014, we then looked for any evidence
for three different interpretations of the phase delay effect; namely:
\bi
\item
That defects are more expensive to fix in later phases; 
\item
That defects become more expensive the more phases they remain; 
\item
That requirements errors are more expensive to fix the later they are found. 
\ei
Our dataset provided no support for any of these phrasings. 
This is significant, as we argue that this paper is (by far)    the largest dataset ever used to explore phase delay (but see ~\cite{Boehm80,Clutterbuck09,Shull02,Royce98} 
for other, much older, much smaller studies).


 


Despite this lack of evidence, the myth remains that phase delay causes increased time to resolve issues.
 Since the phase delay effect has been such a long-lived software engineering truism, as well as one that is accepted by a substantial population of software engineers, it is important to explore possible explanations for our observations.
 
One hypothesized explanation that can be ruled out is that this lack of
a phase delay effect is due to the use of TSP.
  To the contrary, four non-TSP  case studies reported that the expected exponential growth in cost-to-fix was not observed:
  \bi
  \item
  Boehm~\cite{Boehm80} reported a flatter growth rate for small, non-critical projects.
  \item
  Clutterbuck~\cite{Clutterbuck09} showed a flatter cost curve in an agile case study. 
  \item 
  Data from NASA's Johnson Space Flight Center, reported by Shull~\cite{Shull02}, found that the cost to find certain non-critical classes of defects was fairly constant across lifecycle phases (1.2 hours on average early in the project, versus 1.5 hours late in the project). 
  \item
  Royce~\cite{Royce98} reported a flat cost curve for a project that invested heavily upfront in architectural reviews.
\ei
If the lack of phase delay is not due to TSP, then what does cause it?
We surmise that the substantial changes in software development and deployment technologies and practices, since the phase delay effect was first measured, may be responsible for our observations in the TSP data set. Recall that the original data on which the phase delay effect is based, as reported in \S3, are approximately 40 years old. The most recently reported data, from Shull et al. \cite{Shull02}, was published in 2002 but contains substantial retrospective data. Over time, it would be surprising if the investment in better languages, IDEs, and other development technologies had in fact not created some improvement. To pick just one example, we note that much of the data showing the phase delay effect was collected before frequent and unobtrusive auto-updating software was as common as today, which certainly flattens the cost of changes.

%One hypothesized explanation that can be ruled out is that the lack of a phase delay effect is due to using the TSP method. 


The question remains: why did this idea, with so little support, become so widespread in the software engineering literature? No doubt the original evidence was compelling at the time, but much has changed in the realm of software development in the subsequent 40 years. Possibly the concept of phase delay has persisted because, to use Glass's terms on the subject, it seems to be ``just common sense''\cite{glass02}. 
We believe that these results indicate that, in a rapidly changing field such as software engineering, even widely-held rules of thumb must be periodically re-verified. In turns out that common sense may not be so sensible after all.
Progress in the domain of software analytics has made such periodic checks more cost-effective and feasible, and we argue that an examination of local behaviors (rather than simply accepting global heuristics) can be of significant benefit.


 
% \section{Related Work}


% \begin{figure}
% \begin{center}
% \scriptsize\begin{tabular}{|l@{~:~}l|}\hline
% Bug Prediction Dataset &http://bug.inf.usi.ch \\
% Eclipse Bug Data &http://goo.gl/tYKahN \\
% FLOSSMetrics& http://flossmetrics.org \\
% FLOSSMole &http://flossmole.org \\
% IBSBSG& http://www.isbsg.org \\
% ohloh& http://www.openhub.net \\
% PROMISE &http://promisedata.googlecode.com \\
% Qualitas Corpus &http://qualitascorpus.com \\
% Software Artifact Repository &http://sir.unl.edu \\
% SourceForge Research Data &http://zerlot.cse.nd.edu \\
% Sourcerer Project &http://sourcerer.ics.uci.edu \\
% Tukutuku &http://www.metriq.biz/tukutuku \\
% Ultimate Debian Database &http://udd.debian.org\\\hline
% \end{tabular}
% \end{center}
% \caption{Some repositories of software engineering data.}\label{fig:sedata}
% \end{figure}
% The data collected in the SEI TSP databases contains 
% extensive process details
% including a detailed phased breakdown showing what happened at what
% phases of the lifecycle. This makes this data somewhat different to the standard
%  publicly available
% software projects data sets currently available to 
% SE researchers. Most of the \fig{sedata} data sets have software product information
% such as full source code, or summaries of static features.
% A subset of that data contain issue or defect reports and/or
% the time taken to build these systems.  We are unaware
% of any of these having detailed phase-by-phase breakdowns of project data.



\section*{Acknowledgements}
This work was partially funded by an National Science
Foundation grants NSF-CISE 1302169 and CISE 1506586.
\\
We thank Tuma Solutions for providing the Team Process Data Warehouse software in support of this research.  \\
Personal Software Process$\textsuperscript{SM}$, Team Software Process$\textsuperscript{SM}$, and TSP$\textsuperscript{SM}$ are service marks of Carnegie Mellon University.

 
\vspace*{0.5mm}
\scriptsize
\bibliographystyle{plain}
\bibliography{refs} 


\end{document}


% One way to address the absence of general laws is  {\em local learning}.
% Even if general laws
% do not exist, then {\em there exists general methods for finding the best local laws}.
% For example, one of us (Nichols) works extensively with software projects to help
% them tune their local software process in order to better fit their local needs.XXX.

%  Recent studies report that better predictors of the properties of
% software projects (effort and defects) can be generated by first clustering
% (a.k.a. stratifying or contextualizing or localizing) the data then learning
% different models for each cluster~\cite{posnett11,betten14,betta12,yang11,yang13,minku13}.
% Those {\em local learning} approach finds better predictions with lower variance than
% models learned from all the unclustered data~\cite{me12d,me11m}.

% The problem with the local learning is that they it can generate conclusions
% that conflict with strongly-held beliefs of the humans members of the software development team.
% It is an open issue how to handle those conflicts since it is unwise to
% always reject the conclusions of local learning  or the software developers.
% The local learners may be wrong due to incorrect assumptions by the data scientists who configured the learners~\cite{me11e,shull02}.
% On the other hand,  the humans may be wrong due to the cognitive biases described above.
% Experienced data scientists use a cyclic approach where
% (a)~the collected data; and (b)~the learning methods; and (c) the goals of the inquiry are matured  using
% insight offered by humans as they study the
% feedback generated by the learners~\cite{Fayyad96,me11e}.



% the learners' conclusions are discussed with the team in or

% (which may be erroneous
% due to incorrect assumptions 


% We come to this work since  recent results on {\em locality} by Menzies and others


% XXXX In those 171 projects, we observed that it is
%  fastest to fix issues within the same phase $i$ as when they are generated. But in a result that contradicts
%  the phase delay effect, once an issue ``escapes'' phase $i$
%  it so no more expensive to resolve an issue in phase $i+1$ than
%  phase $i+2, i+3$, etc. Further, that increased effort may be quite modest.
%  That is, contrary to established wisdom, once an  issue ``escapes'' a phase, there is no
%  need to  rapidly retire that issue as soon as possible. 
% Our conclusion discusses the implications for our field, and how we might better
% propagated and monitor the wisdom of our field. 



% \begin{figure}
% \begin{center}
% \begin{tabular}{rrl}
% year& \# issues&\\\hline
% 2006 &  44 &\\
% 2007 &  34 &\\
% 2008&  288 &\rule{1mm}{2mm}\\
% 2009&  846 &\rule{3mm}{2mm}\\
% 2010& 1007 &\rule{3mm}{2mm}\\
% 2011& 3273 &\rule{10mm}{2mm}\\
% 2012&18102 &\rule{45mm}{2mm}\\
% 2013&20336 &\rule{50mm}{2mm}\\
% 2014& 3307 & \rule{10mm}{2mm}\\\cline{1-2}
% Total:&47228
% \end{tabular}
% \end{center}
% \caption{This paper studies 47,228 issues recorded 2006 to 2014.}\label{fig:years}
% \end{figure}
