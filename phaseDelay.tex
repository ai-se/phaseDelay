\documentclass{sig-alternate}
\usepackage{multirow}
\usepackage{color}
\usepackage{colortbl}

\usepackage{picture}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}} 
\newcommand{\lucas}[1]{\textcolor{red}{LUCAS: #1}} 
\newcommand{\bill}[1]{\textcolor{blue}{BILL: #1}} 
\newcommand{\carter}[1]{\textcolor{cyan}{CARTER: #1}} 
%\newenvironment{changed}{\par}{\par}

%timm tricks
\newcommand{\bi}{\begin{itemize}[leftmargin=0.4cm]}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
 

\usepackage[shortlabels]{enumitem} 
\usepackage{times}

\usepackage{cite}
\newcommand{\subparagraph}{}
\usepackage{url}
\def\baselinestretch{1}


\setlist{nosep}

\usepackage{colortbl}
 \usepackage[font={small}]{caption, subfig}
\setlength{\abovecaptionskip}{1ex}
 \setlength{\belowcaptionskip}{1ex}

 \setlength{\floatsep}{1ex}
 \setlength{\textfloatsep}{1ex}
\usepackage[compact,small]{titlesec}
\DeclareMathSizes{7}{7}{7}{7} 
\pagenumbering{arabic}
\setlength{\columnsep}{7mm}

\begin{document}

\conferenceinfo{FSE}{'15 Bergamo, Italy}
\title{Revisiting the Truisms of Software Engineering:\\ Does Phase Delay Dramatically Increases  Repair Time?}
\numberofauthors{3}
\author{
\alignauthor
Tim Menzies, \\Carter Pape\\
       \affaddr{CS, NcState, USA}\\
       tim.menzies@gmail.com,\\carterpape@gmail.com
\alignauthor
William Nichols,\\ Forrest Schull\\
\affaddr{SEI, CMU, USA}
wrn,fjshull@sei.cmu.edu
\alignauthor
Lucas \\Layman\\
       \affaddr{Fraunhofer Center,  USA}\\ 
       llayman@fc-md.umd.edu
} 


 
\maketitle
\begin{abstract}
Does
repair time increases dramatically
the longer a defect persists in a system?
This  is very widely belief and the core rational  for
 rejecting   traditional linear development methods
 in favor of more cyclic agile approaches.

This paper shows that this belief is maintained
quite strongly in the industrial software engineering
community (but, in the academic community, somewhat less so).
Yet based on a sample of 
\carter{230} software projects from around the world from 
\bill{2005 to 2013}, we argue that this belief is mostly 
incorrect. Specifically: the median fix time for issues
is just a few minutes and this increases by a small linear amount
the longer the issue remains in the system. 

What was observed was a small number
of defects that take a remarkably long time to resolve
For these issues, if an issue takes more an hour
to resolve then it will usually require many more hours of work.
These long-tail bugs
are more prevalent the longer a project delays issue removal. 
In order to handle such ``long tail'' issues,
we suggest  augmenting software
development groups with additional ``tiger teams'' that are triggered
when issue resolution time increases beyond a certain number of
minutes.
\end{abstract}

% A category with the (minimum) three required fields
\vspace{1mm}
\noindent
{\bf Categories/Subject Descriptors:} 
D.2.8 [Software Engineering]: Product metrics; 

 

\vspace{1mm}
\noindent
{\bf Keywords:} defect prediction, 

\section{Introduction}
Several recent papers call into questions long-held trusims in the field
of software engineering. Devanbu reports that, in practice, the advantage of types languages
is only very minimal. Nagappan et al. found that XXX, contrary to the advice of Dyketra,
it is not necessarily true that ``goto considered harmful'' XXX. More generally,
several recent  papers comment on ``locality'' effects where effects
thold  generally across XXXX
 
Perhaps it is time to take a look at the base premises of our field, particularly in light of the large software analytics data sets newly available for reseaercehrs.

XXX tsp. 40MB . 235 prokects

\section{Motivation}
\subsection{Why Study Triusms?}

Before exploring one particular truism, we digress to discuss the merits of revisiting
old conclusions in software engineering.

There are many examples of writers proposing general laws of software engineering.
Processional societies assume such generalities exist when the offer
 lists of  supposedly general ``best practices'' such as
the IEEE 1012 standard for software verification~\cite{1012}.
Budgen \& Kitchenham seek to reorganize SE research using
general
conclusions drawn from a larger number of studies~\cite{kitch04,budgen09}.
Widely-cited researchers 
list
conclusions that are meant to hold across most SE projects; e.g.
Glass~\cite{glass02}; Jones~\cite{jones07}; Boehm~\cite{boehm00b} and Endres \& Rombach
whose textbook lists ``50 laws of software engineering''~\cite{endres03,rombach11}. 
Endres \& Rombach are especially adamant   that such generalities exist:  
\begin{quote}
Based on {\em repeated and consistent} observations (emphasis added), key lessons of these fields can now be formulated into rules or even laws, providing initial building blocks towards a theoretical foundation that is essential for further research, for teaching and for the practice of software development. 
\end{quote}
The message of this paper is that we need to be very careful before taking action based
on these supposed general laws. Contrary to the claims of Endres \& Rombach,
we found {\em repeated inconsistencies} in the SE literature.
For example,
this papers tests   Endres \& Rombach's claim that  phase delay
 dramatically increases the time to fix issues. We show that (a)~this is one of the most
 strongly held beliefs amongst industrial practitioners and that (b)~it is
 not supported by data from  171 projects from the period 2004 to 2014. 
 This result is one of many similar findings that 
cast doubt on the existence of  ``key lessons  that can be formulated
as general laws of SE'':
\bi
\item
Turhan~\cite{me12d} lists 28 studies with contradictory conclusions
on the relation of OO measures to defects.  Those results
 directly  contradict some of the laws listed by 
Endres \& Rombach~\cite{endres03}.
\item
Ray et al.~\cite{ray2014lang} tested if   strongly typed languages
predict for better code quality. In  728 projects,
they found  only a modest benefit in strong typing
(and caution that that effect may actually be due to other  conflating factors).
\item
Fenton \& Neil~\cite{fenton00,fenton00b}   critique the truism that
``pre-release fault rates for software
are a predictor for post-release failures'' (as claimed by~\cite{dunsmore88},
amongst others). For the systems described in~\cite{fenton97}, they
show that software modules that were highly fault-prone
prior to release revealed very few faults after release.
\item
Meyer claims that   object-oriented (OO) encapsulation will
reduce error rates in software~\cite{Meyer1988}.  Yet empirical results suggest
that debugging an OO program is many times harder and
longer than debugging a standard procedural program~\cite{hatton98}.
\item
A truism of visual programming is that ``visual
representations are inherently superior to mere textual representations''.
A review by Menzies suggests that the available
evidence for this claim is hardly conclusive~\cite{me00v}. 
\ei
To be fair to the software engineering community, SE is not the only
field where practitioners hang on to persistent beliefs, even if the evidence
for those beliefs is not strong.
The medical profession applies  many practices based on studies
that have been disproved. For example,
a  recent article
in the Mayo Clinic Proceedings~\cite{prasad13} found  
146 medical practices based on studies 
in year $i$, but which were  reversed by subsequent trials within years $i+10$.
Even when the evidence for or against a treatment or intervention is clear, medical providers and patients may not accept it~\cite{aschwanden10}.
Aschwanden warns:
\begin{quote}
Cognitive biases - such as motivated reasoning (all of us want to believe that the things we do make a difference), base rate neglect (failing to pay attention to what happens in the absence of the intervention), and confirmation bias (the tendency to look for evidence that supports what you already know and to ignore the rest) - also influence how we process information~\cite{aschwanden15}.
\end{quote}

The cognitive issues that complicate medicine are also found in software engineering.
According to Passos et al.~\cite{passos11}, commercial developers
are all too willing to believe in general
truisms.
They  warn that developers
usually assume that the truisms they learn from a few past
projects are general to 
all their future projects:
\begin{quote}\label{q:pass}
...the past experiences were taken into account without 
much consideration for their context~\cite{passos11}.  
\end{quote}
The results of Jorgensen \& Gruschke support the findings if Passos et al. They repor that 
  commercial estimation ``gurus'' rarely use lessons
  from past projects to improve their future estimates~\cite{jorgensen09}. 
When engineers
  fail to revise their beliefs, this leads to poor
 estimates  (see examples in~\cite{jorgensen09}).
%% This leads to  detrimental
%% situations where groups or 
%% individuals working on the same project have conflicting beliefs, which they never debate
%or refine or unify.

In summary, just like medicine, our field suffers when
 software engineers do  revise old beliefs.  Therefore, papers like this one
 that retests old beliefs are vital.
\subsection{Why Study Phase Delay?}

It is no understatement to say that 
concerns with  phase delay have 
a major  motivator for changing
SE practices. Basili and Boehm~\cite{boehm01}  commented in
2001 that, since the 1980s, the phase delay
``has been a major driver in focusing
industrial software practice on thorough
requirements analysis and design,
on early verification and validation, and
on up-front prototyping and simulation
to avoid costly downstream fixes.''

\begin{center}
\includegraphics[width=3.3in]{beckB4.png}

\includegraphics[width=3.3in]{beckAFTER.png}
\end{center}

Also, concerns with the phase delay  had
 a significant impact on the history of agile programming. Kent Beck is
 particularly eloquent  

Under certain circumstances, the exponential rise in the cost of changing software over time can be flattened. If we can flatten the curve, old assumptions about the best way to develop software no longer hold.

One of the universal assumptions of software engineering is that the cost of changing a program rises exponentially over time. I can remember sitting in a big linoleum-floored classroom as a college junior and seeing the professor draw on the board the curve found in Figure 1.

The software development community has spent enormous resources in recent decades trying to reduce the cost of change—better languages, better database technology, better programming practices, better environments and tools, new notations.

What would we do if all that investment paid off? What if all that work on languages and databases and whatnot actually got somewhere? What if the cost of change didn't rise exponentially over

This is one of the premises of XP. It is the technical premise of XP. If the cost of change rose slowly over time, you would act completely differently from how you do under the assumption that costs rise exponentially. You would make big decisions as late in the process as possible, to defer the cost of making the decisions and to have the greatest possible chance that they would be right. You would only implement what you had to, in hopes that the needs you anticipate for tomorrow wouldn't come true. You would introduce elements to the design only as they simplified existing code or made writing the next bit of code simpler.
￼
time, but rose much more slowly, eventually reaching an asymptote? What if tomorrow's software engineering professor draws Figure 3 on the board?

The cost to fix a problem in a piece of software rises exponentially over time. A problem that might take a dollar to fix if you found it during requirements analysis might costs thousands to fix once the software is in production."

I resolved then and there that I would never let a problem get through to production. No sirree, I was going to catch problems as soon as possible. I would work out every possible problem in advance. I would review and crosscheck my code. No way was I going to cost my employer \$100,000.


\section{Related Work}

In their text Romabach laws

Kent Beck's original extreme programming text is big on this.

Historical  evidence for this dataes back to some studies in the 1970s. To the best of
our knowledge, not updated since. Regardless, as discussed in the next section, this

\input{lucasRelatedWork}

\section{lucasSurvey}

\input{lucas}


\section{billTSP}

\section{carterCharts}

\input{carter}

\section{Data}

\begin{figure}[!t]

%\renewcommand{\baselinestretch}{0.8}
\scriptsize
\begin{center}
\begin{tabular}{r|rrr|ll}
  Sample&\multicolumn{3}{c|}{Percentiles}\\ 
size & 25th & 50th & 75th & Phase injected & Phase removed\\\hline
57& 6& 16& 47&  BeforeDevelopment&Code\\
72& 2& 4& 11&  BeforeDevelopment&CodeInspect\\
165& 6& 18& 56&  BeforeDevelopment&Test\\
50& 6& 20& 37&  BeforeDevelopment&IntTest\\
42& 8& 28& 65&  BeforeDevelopment&SysTest\\\hline

66& 2& 6& 15&  Planning&Planning\\
41& 1& 5& 16&  Planning&ReqtsReview\\\hline

23& 0& 1& 3&  Reqts&Reqts\\
245& 6& 13& 23&  Reqts&ReqtsReview\\
289& 7& 16& 30&  Reqts&ReqtsInspect\\
32& 6& 21& 40&  Reqts&Design\\
49& 2& 7& 24&  Reqts&DesignInspect\\\hline
 

94& 2& 5& 10&  HLD&HLDReview\\
133& 3& 8& 19&  HLD&HLDInspect\\
33& 2& 5& 17&  HLD&Design\\\hline



37& 2& 4& 6&  Design&Design\\
455& 11& 24& 53&  Design&DesignInspect\\
218& 5& 14& 30&  Design&Code\\
166& 4& 11& 22&  Design&CodeInspect\\
542& 14& 31& 60&  Design&Test\\
67& 6& 15& 40&  Design&IntTest\\
92& 10& 25& 53&  Design&SysTest\\\hline

53& 2& 4& 9&  DesignInspect&DesignInspect\\
38& 1& 3& 9&  DesignInspect&Code\\\hline

126& 4& 12& 31&  Code&Code\\
459& 11& 24& 45&  Code&CodeInspect\\
461& 11& 26& 55&  Code&Test\\
348& 10& 26& 62&  Code&IntTest\\
230& 8& 18& 39&  Code&SysTest\\
71& 2& 11& 28&  Code&AcceptTest\\\hline

64& 2& 4& 10&  CodeInspect&CodeInspect\\
57& 2& 5& 13&  CodeInspect&Test\\\hline



110& 3& 10& 23&  Test&Test\\
66& 2& 4& 12&  Test&QualTest\\\hline


32& 1& 2& 4&  IntTest&QualTest\\
36& 1& 11& 30&  IntTest&IntTest\\
21& 1& 2& 5&  IntTest&SysTest\\
 \end{tabular}
\end{center}
\caption{Distribution of fix times seen in SEI TSP data.}
\label{fig:faw}
\end{figure}
 
 
\begin{figure}
\begin{center}
\begin{tabular}{rrl}
year& \# issues&\\\hline
2006 &  44 &\\
2007 &  34 &\\
2008&  288 &\rule{1mm}{2mm}\\
2009&  846 &\rule{3mm}{2mm}\\
2010& 1007 &\rule{3mm}{2mm}\\
2011& 3273 &\rule{10mm}{2mm}\\
2012&18102 &\rule{45mm}{2mm}\\
2013&20336 &\rule{50mm}{2mm}\\
2014& 3307 & \rule{10mm}{2mm}\\\cline{1-2}
Total:&47228
\end{tabular}
\end{center}
\caption{This paper studies 47,228 issues recorded 2006 to 2014.}\label{fig:years}
\end{figure}
\begin{figure*}[!t]
\begin{center}
\includegraphics[width=6in]{waterfall2.png}
\end{center}
\caption{Different approaches to software development:  waterfall and agile (bottom left).}
\end{figure*}


\begin{figure*}[!t]

\renewcommand{\baselinestretch}{0.7} 
\begin{center}
\begin{tabular}{ll|r|rl}
            &                  & Sample\\
Phase injected & Phase removed & size & \multicolumn{2}{l}{Scale up, w.r.t. to first phase}
\input{deltas}
\end{tabular}
\end{center}
\caption{50th percentile (median) scale ups  for  time to resolve issues (taken from \fig{raw}).}
\label{fig:scale}
\end{figure*}



In these results, we checked if a small number of bugs are most expensive. 
To check this:
\bi
\item
We repeated the analysis that generated \fig{scale},
but instead of looking at the 50th percentile, we displayed the scale up factors
\item 
We only
checked the Design and Code scale up results since, from \fig{scale}, it is clear these
have the most examples of longest phase delays.
\ei 
The  results are shown in \fig{scale90} and these
results are somewhat different for Design and Coding issues:
\bi 
\item For Design issues,  these have the same
general form as the 50th percentile results. That is, while it it certainly faster
to remove thing sin the phase where they are created, once we leave that phase
it does not seem to matter much how many phases we wait before fixing the issue.
\begin{itemize}
\item For Coding issues, the seems little impact of phase delay on the time
required to fix.
\end{itemize}


\begin{figure*}[!t]

\renewcommand{\baselinestretch}{0.7} 
\begin{center}
\begin{tabular}{ll|r|rl}
            &                  & Sample\\
Phase injected & Phase removed & size & \multicolumn{2}{l}{Scale up, w.r.t. to first phase}
\input{deltaBig}
\end{tabular}
\end{center}
\caption{90th percentile scale ups  for  time to resolve issues .}
\label{fig:scale90}
\end{figure*}


The data used in this work shirai14


The Team Software ProcessSM (TSPSM) provides a
framework that teams use to collect software process data in real
time, using a defined disciplined process.
In particular, three common features of the projects in our SEI TSP data set  are coaches,
peer review, and  personnel reviews:
\bi 
\item
A ``coach'' is the team member  authorized to submit project data
(before submission,
these coaches check the data for obvious errors);
\item 
``Personnel review'' is a technique taken  from the Personal Software
ProcessSM (PSPSM);
\item 
``Peer review'' is a standard technique in
traditional software engineering.
\ei
PSPSM encourages developers to continually make and review their personnel estimates
about their day-to-day tasks, then compare those estimates against the actual development effort.
In this way, developers can acquire a more realistic understanding of their work behaviour.
As to peer review,  Basili and Boehm write  commented in 2001~\cite{boehm01} 
that peer reviews can catch over half the defects introduced into a system.
Peer review can be conducted on any artifact generated anywhere in the software
lifecycle and can quickly be adapted to new kinds of artifacts.


\begin{figure}
\begin{center}
\scriptsize\begin{tabular}{|l@{~:~}l|}\hline
Bug Prediction Dataset &http://bug.inf.usi.ch \\
Eclipse Bug Data &http://goo.gl/tYKahN \\
FLOSSMetrics& http://flossmetrics.org \\
FLOSSMole &http://flossmole.org \\
IBSBSG& http://www.isbsg.org \\
ohloh& http://www.openhub.net \\
PROMISE &http://promisedata.googlecode.com \\
Qualitas Corpus &http://qualitascorpus.com \\
Software Artifact Repository &http://sir.unl.edu \\
SourceForge Research Data &http://zerlot.cse.nd.edu \\
Sourcerer Project &http://sourcerer.ics.uci.edu \\
Tukutuku &http://www.metriq.biz/tukutuku \\
Ultimate Debian Database &http://udd.debian.org\\\hline
\end{tabular}
\end{center}
\caption{Some repositories of software engineering data.}\label{fig:sedata}
\end{figure}
The data collected in the SEI TSP databases contains 
extensive process details
including a detailed phased breakdown showing what happened at what
phases of the lifecycle. This makes this data somewhat different to the standard
 publicly available
software projects data sets currently available to 
SE researchers. Most of the \fig{sedata} data sets have software product information
such as full source code, or summaries of static features.
A subset of that data contain issue or defect reports and/or
the time taken to build these systems.  We are unaware
of any of these having detailed phase-by-phase breakdowns of project data.




The phased used in this paper are shown in \fig{waterfall}. Note that, in that figure:
\bi 
\item
Several  phases have the same  sub-activities of {\em review} and {\em inspect}\footnote{\bill{plz distinguish review and inspect. does one usually happen first?}}
\item Testing is divided into several statges \footnote{\bill{need a one
line description of test vs qualtest vs inttest vs systemTest vs AcceptTest. I tries some words in the Key to \fig{sedata}. dont know if i got it right.}}
\ei 
Also shown in \fig{waterfall} (bottom left) is the standard definition of an agile process~\cite{boehmturner03}:
Given some
some backlog of tasks, wgeb teams complete their current tasks, they select the next task(s) to complete. That selection process may use a variety of criteria
to prioritize which  tasks are selected (for more details on that selection process, see~\cite{me09j,port08,boehmturner03}). Tasks are completed in ``sprints'' that can last hours,
days, but rarely not more than weeks. Each day meet for brief ``scrum'' sessions to assess (and possibly alter) their current progress on the goals of the sprint.  
Agile teams race to generate releases
(in the continuous release model, releases can be generated on a daily basis, or even faster).  
Experience gained from those releases informs the discussion in the daily scrums which, in turn,
can inform the team's decisions on how to select and implement the next set of tasks for next sprint.
In fact, that experience can result in changing some/all of the tasks in the current backlog. 

TSP is not antithetical to agile--  indeed a TSP-waterfall style project can adopt aspects
of agile.  For example the agile loop  could be applied
over one or more of the phases shown in the long waterfall chain of \fig{waterfall}. Some
TSP teams adopt something like test-driven-development\footnote{TDD is an agile-method
where the ``test'' is the primary driver of the design. The tests are written first,
then the code to support those tests. TDD proceeds in three steps: red (where there
are broken tests); green (where tests are passing); and, possibly, refactor (where
the code is re-organized based on feedback from those tests~\cite{fraser03}.} where
reviews are scheduled after testing. Some of the groups in the SEI TSP data used that approach but
they do not effect the main conclusions of this paper:
\bi 
\item They were strongly in the minority\bill{any numbers on this?};
\item Most of our phase delay data comes from much early in the waterfall model
of \fig{waterfall}.
\ei 
That said, there are some very ``un-agile'' aspects
of the processes used by the   projects in the TSP SEI data.
A TSP participant spends much time reflecting on the project,
undisturbed by the behaviour of the  executables.
In fact,
it can be days/weeks
before TSP participants gain  feedback from executing code, for the following reasons:  
\bi 
\item
For TSP projects, the times spent in the of a design activity/phase is   
approximately as much effort as the coding phase.  
\item Our TSP teams did not
 combine development and  testing. 
 \item  The TSP projects studied here offer new releases at least every three months, but
often much longer that that\bill{any numbers on that}. Note that this is a far slower
release schedule than seen in, say, continuous integration projects.  
\item The time devoted to personnel review'' and peer review (defined above)
is  about 50\% as much effort as the previous construction phase activity.
 
\ei

Having presented all that, we can now present the main point of this section:
\bi 
\item The results section of this paper fails to find that  phase delay causing dramatic increases
in time to fix issues;
\item This lack-of-phase-delay effect {\em cannot} be explained away just by  saying that
the projects in this sample adopted something like  agile methods to reduce re-work costs.
\ei  




\section{Data}

The Software Engineering Institute (SEI) at
Carnegie Mellon University has collected data from organizations
that have adopted TSP. 
Our efforts focused on data collected by projects that launched
after 2009 and used an automatic data recording tool. 
Due to confidentiality restrictions, we cannot offer too many details
on those project. That said, there were less than a dozen United States
Department of Defence projects. Also, the majority of
the projects were weither web portals or banking systems in the US, South Africa, and Mexico. 
There were also some  medical devices (US, France, Japan, and Germany) and a few from a commercial 
computer-aided design system (developed by a world wide distributed team). 


Data collected included time logs, defect
logs, added and modified size logs, and work
breakdown strcture logs.
The projects
were mostly small to medium, with a median duration of 46 days
and a maximum duration of 90 days. Median team size was 7
people, with a maximum of 40. Hence,  total development 
effort\footnote{
Effort =  duration*teamSize/workDaysPerYear.} for
these projects ranged from 1.3 years (median) to 14.3 years (max).

In these logs
the log records include work start time, work end time, delta
work time, and interruption time. Software engineers are often
interrupted by meetings, requests for technical help, reporting, and
so forth. These events are recorded, in minutes, as interruption
time. In this paper, when we report ``time to resolve an
issue'', we show the difference between the start and end times
of a work session, with any interruption time subtracted (the
difference in times, minus the interruptions).  

As of January 2014, the SEI TSP database contained data from 109
TSP projects. The projects started between July 2009 and
September 2013; they included 34 teams and 309 people. Among
the database fact tables, the time store contains 103,023 time logs,
18,408 defect logs, and 7,464 size logs\footnote{\bill{we need to 
update this table}}.  

A common property of real-world data sets is the presence
of noisy entries (superfluous  or spurious data). 
The level of noise can be quite high. As reported
in \cite{shepperd12}, around
10\% to 30\%
of the records in the NASA MDP defect data sets are
affected by noise. Nichols et al.~\cite{shirai14}  report that
the noise levesl in the SEI TSE data are smaller than those seen
in other data sets. They found in the SEI TSP data that:\bi 
\item
4\% of the data was incorrect such as  null values of illegal formats;
\item  2\% of the data has inconsistencies such as timestamps
where the stop time was before the start time;
\item 3\% of the data had data that was not credible
such as tasks listed in one day that took like than six hours.
\ei 
That said, certain semantic features of the SEI TSP data should be noted.
Firstly, in the current TSP collection tool, 
fix times are only the developer time for the developer walking through the phases of \fig{waterfall}.
We are currently tracking the fix time for post-release issues (e.g. those raised during  acceptance test and later
product life cycle). So far, in that post-release data,  we have not detected
a dramatic phase escalation effect (but at this time, we have nothing definitive comment on that matter).

\bill{somewhere you have one note on \underline{find} and fix times.  for this paper, we need just fix times. but is there
anything we need to fret about re \underline{find} times?}

The majority of the teams follow a more linear process within a single feature of physical component (usually no more than a week or two work in total). Unlike some agile teams, *most*TSP teams will not conflate development and unit test.

The starting point for this paper were the times required to fix 47,228 issues.
All data was collected
by the Software Engineering Institute in the period 2006 to 2014 (see \fig{years}).
To the best of our knowledge, this is the largest sample of ``time to fix issues''
yet analyzed. 

This data was collected using the   methodology described by
Watts Humphrey's Team Software Process~\cite{tsp00}. TSP is an extension of Humphrey's early
work on Personnel Software Process~\cite{psp05}.  PSP encourages developers to continually make estimates
about their day-to-day tasks, then compare those estimates against the actual development effort
(in this way, developers can acquire a more realistic understanding of their work behaviour).

In all data, was collected from 172 separate projects. Given Beck's comments (above) about phase delay motivating agile processes, we take care to identify the differences of these 
projects to agile development projects. An alternate, more traditional approach is a ``macro linear''
approach such as the ``waterfall methodology'' that develops code after an extensive
pre-planning stage:
 

In the original paper defining and 
critiquing ``waterfall'', Royce defines a set of assessment and feedback processes
such that 


care to contrast these 
The majority of the teams follow a basically linear process within a single feature of physical component (usually no more than a week or two work in total). Unlike some agile teams, *most*TSP teams will not conflate development and unit test.  This is usually pretty clear from the task  phase sequencing. A few teams choose to follow a Test Driven Development like process in which code review or inspection will follow test. In a more traditional “micro-linear” TSP approach, unit test follows inspections. 


Our data reporting the time required
to resolve issues in phase $j$ that there were introduced in phase $i, i<j$.



The defect typ that we studied were
\be 
\item design, compile, test, or other support system problems = Environment
procedure calls and reference, I/O, user formats = Interface
structure, content = Data
comments, messages = Documentation
spelling, punctuatio typos, instruction formats = Syntax
logic, pointers, loops, recursion, computation, function defects = Function
error messages, inadequate checks = Checking
change management, library, version control = Build, package
declaration, duplicate names, scope, limits = Assignment
configuration, timing, memory = System
\ee
\section{Conclusion}

\section*{Acknowledgements}
This work was partially funded by an National Science
Foundation grant NSF-CISE 1302169.

\clearpage
\vspace*{0.5mm}
\scriptsize
\bibliographystyle{plain}
\bibliography{refs} 


\end{document}


One way to address the absence of general laws is  {\em local learning}.
Even if general laws
do not exist, then {\em there exists general methods for finding the best local laws}.
For example, one of us (Nichols) works extensively with software projects to help
them tune their local software process in order to better fit their local needs.XXX.

 Recent studies report that better predictors of the properties of
software projects (effort and defects) can be generated by first clustering
(a.k.a. stratifying or contextualizing or localizing) the data then learning
different models for each cluster~\cite{posnett11,betten14,betta12,yang11,yang13,minku13}.
Those {\em local learning} approach finds better predictions with lower variance than
models learned from all the unclustered data~\cite{me12d,me11m}.

The problem with the local learning is that they it can generate conclusions
that conflict with strongly-held beliefs of the humans members of the software development team.
It is an open issue how to handle those conflicts since it is unwise to
always reject the conclusions of local learning  or the software developers.
The local learners may be wrong due to incorrect assumptions by the data scientists who configured the learners~\cite{me11e,shull02}.
On the other hand,  the humans may be wrong due to the cognitive biases described above.
Experienced data scientists use a cyclic approach where
(a)~the collected data; and (b)~the learning methods; and (c) the goals of the inquiry are matured  using
insight offered by humans as they study the
feedback generated by the learners~\cite{Fayyad96,me11e}.



the learners' conclusions are discussed with the team in or

(which may be erroneous
due to incorrect assumptions 


We come to this work since  recent results on {\em locality} by Menzies and others
