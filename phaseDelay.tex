\documentclass{sig-alternate}
\usepackage{multirow}
\usepackage[usenames, dvipsnames]{color}
\usepackage{colortbl}

\usepackage{picture}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}} 
\newcommand{\lucas}[1]{\textcolor{red}{LUCAS: #1}} \newcommand{\tim}[1]{\textcolor{orange}{TIM: #1}}
\newcommand{\bill}[1]{\textcolor{blue}{BILL: #1}} 
\newcommand{\carter}[1]{\textcolor{cyan}{CARTER: #1}} 
\newcommand{\sei}[1]{\textcolor{RedViolet}{BILL/FORREST: #1}} 
\newcommand{\todo}[1]{\textcolor{Maroon}{TODO: #1}} 
%\newenvironment{changed}{\par}{\par}

%timm tricks
\newcommand{\bi}{\begin{itemize}[leftmargin=0.4cm]}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
 

\usepackage[shortlabels]{enumitem} 
\usepackage{times}

\usepackage{cite}
\newcommand{\subparagraph}{}
\usepackage{url}
\def\baselinestretch{1}


\setlist{nosep}

\usepackage{colortbl}
 \usepackage[font={small}]{caption, subfig}
\setlength{\abovecaptionskip}{1ex}
 \setlength{\belowcaptionskip}{1ex}

 \setlength{\floatsep}{1ex}
 \setlength{\textfloatsep}{1ex}
\usepackage[compact,small]{titlesec}
\DeclareMathSizes{7}{7}{7}{7} 
\pagenumbering{arabic}
\setlength{\columnsep}{7mm}

\begin{document}

\conferenceinfo{FSE}{'15 Bergamo, Italy}
\title{Revisiting the Truisms of Software Engineering:\\ Does Phase Delay Dramatically Increases  Repair Time?}
\numberofauthors{3}
\author{
\alignauthor
Tim Menzies, \\Carter Pape\\
       \affaddr{CS, NcState, USA}\\
       tim.menzies@gmail.com,\\carterpape@gmail.com
\alignauthor
William Nichols,\\ Forrest Schull\\
\affaddr{SEI, CMU, USA}
wrn,fjshull@sei.cmu.edu
\alignauthor
Lucas \\Layman\\
       \affaddr{Fraunhofer Center,  USA}\\ 
       llayman@fc-md.umd.edu
} 


 
\maketitle
\begin{abstract}
Does
repair time increases dramatically
the longer a defect persists in a system?
This  is very widely belief that is used to justify 
radical changed to    traditional software development
 processes.

This paper shows that this belief is maintained
quite strongly in the industrial software engineering
community (but, in the academic community, somewhat less so).
Yet based on a sample of 
171 software projects from around the world from 
\bill{2005 to 2013}, we argue that this belief is mostly 
incorrect. Specifically: the median fix time for issues
is just a few minutes and this does not tend to increase
the longer the issue remains in the system. 

%This result raises the question: how did myths like phase delay survive so long  without  critical reassessment.  As a community, we need to reflect more on research practices and on how (and when) we reflect on the prominent hypotheses of our field.
\end{abstract}

% A category with the (minimum) three required fields
\vspace{1mm}
\noindent
{\bf Categories/Subject Descriptors:} 
D.2.8 [Software Engineering]: Process metrics.

 

\vspace{1mm}
\noindent
{\bf Keywords:} phase delay, time to fix, agile, waterfall.

\section{Introduction}
In 2013-2014, the time of 
eleven  million programmers~\cite{pettey14} and
half a trillion dollars~\cite{avram14} was spent on information technology.
Such a large and growing effort should be managed and optimized via  well-researched conclusions.  
It is standard practice
in other fields such as medicine,
to continually revisit old conclusions~\cite{prasad13}. Software engineering should do the same lest
ill-founded  exert an undue influence on how we build software.  

Accordingly, this paper revisits
the {\em phase delay effect}
i.e.,  the time required to resolve an issue in a software project increases dramatically 
the longer it remains in the system. 
Phase delay has long been
a    motivator for major changes to
SE practices. Basili \& Boehm comment that since the 1980s,  phase delay:
\begin{quote}
...has been a major driver in focusing
industrial software practice on thorough
requirements analysis and design,
on early verification and validation, and
on up-front prototyping and simulation
to avoid costly downstream fixes~\cite{boehm01}.
\end{quote}
Basili and Boehm are not alone in this assessment of the significance of the phase delay effect. Our surveys of industrial practitioners (shown in \S\ref{survey}) reveal the phase-delay effect to be
the single most strongly held belief amongst commercial SE engineers.
Previously~\cite{me08a}, we have constructed return-on-investement models
for a common practice amongst U.S. government contractors: 
independent verification \& validation. IVV has been criticized as
ineffective since IVV engineers, who many  unfamiliar with the details of the project, are required to pass judgement on the code.
Hence,  the IVV team may find the same or fewer bugs
as the developers,
On the other hand, IVV has been defended
as follows:  when the IVV team finds bugs earlier that the developers then, due to the phase delay effect, this saves the overall cost of the project. Note that the strength of this argument diminishes
if fixing later-phase issues is not significantly more expensive than fixing them earlier. 



% Also, as discussed later in this paper, concerns over phase delay were a major driver in the development of agile software processes.

In this paper, we investigate two hypotheses:

\bi
    \item $H_1$: The cost-to-fix of defects monotonically increases across phases.
    \item $H_2$: The cost-to-fix of defects monotonically increases as the phase delay increases.
\ei
The rest of the paper investigates these two hypotheses. First, we conduct a survey of practitioners and researchers to gauge their belief in the phase delay effect. Second, we  analyze 171 software  projects developed in the period \bill{2004 to 2014}.  To the best of our knowledge, this is the largest study on phase delay yet seen in the literature.

% The conclusions of this paper is that it is erroneous to believe in phase delay.
% When we looked for supporting evidence for that effect, we found that:
% \be 
% \item The evidence for that effect in the historical literature is very weak (to say the least);
% \item  In  a large sample of   contemporary software projects, there is no evidence for phase delay.
%  \ee
% The rest of this paper present evidence for the above two points. 



% The first point is  documented via  a literature survey and the second point is demonstrated
% via an analysis of  
 
 
The conclusions of this paper are two fold. First, the phase delay effect does not hold in the large body of software projects analyzed. Second, commonly-held beliefs, such as phase delay, must be periodically revisited empirically in order for researchers to provide accurate and up-to-date guidance for the practitioner community.

% the SE community needs to stop
% justifying changes to SE
% practices via the phase delay effect. Secondly,   myths like phase delay should not have lasted decades
% in the literature without  critical reassessment.  As a community, we need to reflect more on our
% research practices and on how (and when) we reflect and revisit the prominent hypotheses of our field.


\section{Motivation}
\subsection{Why Reassess Existing Triusms?}

Before exploring the commonly-held belief of the {\em phase delay effect}, we digress to discuss the merits of revisiting
old conclusions in software engineering.


Belief in general
principles of software
engineering are common to both research and practice. Processional societies assume such generalities exist when they offer
 lists of supposedly general ``best practices'' such as
the IEEE 1012 standard for software verification~\cite{1012}.  Endres \& Rombach's comment that 
\begin{quote}
Based on repeated and consistent observations, key lessons of these fields can now be formulated into rules or even laws, providing initial building blocks towards a theoretical foundation that is essential for further research, for teaching and for the practice of software development~\cite{endres03,rombach11}
\end{quote}
 Endres \& Rombach offer dozens of lessons of software engineering~\cite{rombach11}.
 Many other 
widely-cited researchers  do the same; e.g.
Glass~\cite{glass02}; Jones~\cite{jones07}; Boehm~\cite{boehm00b}.
Budgen \& Kitchenham seek to reorganize SE research using
general
conclusions drawn from a larger number of studies~\cite{kitch04,budgen09}.

Nevertheless, there are many empirical  findings that 
cast doubt on the existence of  ``key lessons  that can be formulated
as general laws of SE'':
\bi
\item
Turhan~\cite{me12d} lists 28 studies with contradictory conclusions
on the relation of OO measures to defects.  Those results
 directly  contradict some of the laws listed by 
Endres \& Rombach~\cite{endres03}.

\item
Ray et al.~\cite{ray2014lang} tested if   strongly typed languages
predict for better code quality. In  728 projects,
they found  only a modest benefit in strong typing
(and caution that that effect may actually be due to other  conflating factors).
\item
Fenton \& Neil~\cite{fenton00,fenton00b}   critique the truism that
``pre-release fault rates for software
are a predictor for post-release failures'' (as claimed by~\cite{dunsmore88},
amongst others). For the systems described in~\cite{fenton97}, they
show that software modules that were highly fault-prone
prior to release revealed very few faults after release.
\item
Meyer claims that   object-oriented (OO) encapsulation will
reduce error rates in software~\cite{Meyer1988}.  Yet empirical results suggest
that debugging an OO program is many times harder and
longer than debugging a standard procedural program~\cite{hatton98}.
\item A truism of visual programming is that ``visual
representations are inherently superior to mere textual representations''. A review by Menzies suggests that the available
evidence for this claim is hardly conclusive~\cite{me00v}. 
\item Numerous recent {\em local learning} results compare single models
learned from all available data to multiple models learned from clusters within the data~\cite{betten14,yang11,yang13,minku13,me12d,me11m,betta12,posnett11}.
A repeated result in those studies in that the local models generated the better effort
and defect predictions (better median results,
lower variance in the predictions).
 \item 
Other work studies  found dozens of single
models that performed worse that {\em committees of models}, all with different opinions,
that contribute to an {\em ensemble} prediction of software development effort~\cite{kocaguneli2012value,azhar13}.
\ei
%To be fair to the software engineering community,
Software engineering is not the only
field where practitioners hang on to persistent beliefs, even if the evidence
for those beliefs is not strong.
The medical profession applies  many practices based on studies
that have been disproved. For example,
a  recent article
in the Mayo Clinic Proceedings~\cite{prasad13} found  
146 medical practices based on studies 
in year $i$, but which were  reversed by subsequent trials within years $i+10$.
Even when the evidence for or against a treatment or intervention is clear, medical providers and patients may not accept it~\cite{aschwanden10}.
Aschwanden warns:
\begin{quote}
Cognitive biases - such as motivated reasoning (all of us want to believe that the things we do make a difference), base rate neglect (failing to pay attention to what happens in the absence of the intervention), and confirmation bias (the tendency to look for evidence that supports what you already know and to ignore the rest) - also influence how we process information~\cite{aschwanden15}.
\end{quote}

The cognitive issues that complicate medicine are also found in software engineering.
According to Passos et al.~\cite{passos11}, commercial developers
are all too willing to believe in general
truisms.
They  warn that developers
usually assume that the truisms they learn from a few past
projects are general to 
all their future projects:
\begin{quote}\label{q:pass}
...the past experiences were taken into account without 
much consideration for their context~\cite{passos11}.  
\end{quote}
The results of J{\o}rgensen \& Gruschke~\cite{jorgensen09} support the findings if Passos et al. They report that 
  supposed software engineering    ``experts'' rarely use lessons
  from past projects to improve their future reasoning~\cite{jorgensen09}. 
 They note that
when the experts
  fail to revise their beliefs, this leads to poor
 conclusions and software projects  (see examples in~\cite{jorgensen09}).
%% This leads to  detrimental
%% situations where groups or 
%% individuals working on the same project have conflicting beliefs, which they never debate
%or refine or unify.

In summary, just like medicine, our field suffers when
 software engineers do  revise old beliefs.  Therefore, it is important
 to regularly  reexamine    old beliefs.
 
\subsection{Why Study Phase Delay?}
\todo{\begin{itemize}
\item Given the Intro and Motivation, this section should focus on the influence of phase delay in software engineering milieux
\item Need to tone down the reliance on Beck. The important Beck point is that eliminating the cost-to-fix curve (resulting from phase delay) is one of the main goals of agile. His observations on new technologies are good too.
\item Need to cite other popular sources of this information, e.g., McConnell's "Ounce of Prevention" article~\cite{mcconnell01}.
\item Robert Glass's~\cite{glass02} book is very popular and contains an entry on this point
\item Google Scholar Citations for phase delay works Boehm88~\cite{boehm88} (673), Boehm \& Basili~\cite{boehm01} (761), 
\end{itemize}}

Due to its prominence in the field of software engineering,
this paper studies phase delay. As mentioned in the introduction, concerns about phase delay have
been used for decades to motivate major changes to software engineering practices. For example,
in 2000, Kent Beck used phase delay to motivate his call for ``extreme programming'' (a set of techniques
that evolved into contemporary agile and DevOps practices). Beck writes:
\begin{quote}
One of the universal assumptions of software engineering is that the cost of changing a program rises exponentially over time. I can remember sitting in a big linoleum-floored classroom as a college junior and seeing the professor draw on the board the curve found in \fig{curve1}.

The cost to fix a problem in a piece of software rises exponentially over time. A problem that might take a dollar to fix if you found it during requirements analysis might costs thousands to fix once the software is in production.

I resolved then and there that I would never let a problem get through to production. No sirree, I was going to catch problems as soon as possible. I would work out every possible problem in advance. I would review and crosscheck my code. No way was I going to cost my employer \$100,000.~\cite{beck00}
\end{quote}

\begin{figure}
 \includegraphics[width=3.3in]{beckB4.png}
 \caption{Projects exhibiting phase delay. From~\cite{beck00}.}\label{fig:curve1}
 \end{figure}
\begin{figure}
 \includegraphics[width=3.3in]{beckAFTER.png}
 \caption{Projects without   phase delay. From~\cite{beck00}.}\label{fig:curve2}
\end{figure}

\begin{figure}[!t]
\scriptsize
\begin{center}
\begin{tabular}{l@{~}|l@{~}|r@{~}r@{~}|r@{~}l}
          &        & \multicolumn{2}{c|}{Time to}  &\\
          &        & \multicolumn{2}{c|}{resolve} &\\
Injection & Removal& initial & now & \multicolumn{2}{l}{Scale up w.r.t initial} \\\hline
Phase $i$&Phase $i$ & 1 & 1 & 1 & **\\
Phase $i$ &Phase $i+1$ & 1 & 2& 2 & **** \\
Phase $i$ &Phase $i+2$ & 1 & 4& 4 &******** \\
Phase $i$ &Phase $i+3$ & 1 & 8 & 8 & ****************  
  \end{tabular}\end{center}
 \caption{Projects data consistent with
 the phase delay effect of \fig{curve1}.}\label{fig:curve1a}
\end{figure}

\begin{figure}[!t]
\scriptsize
\begin{center}
\begin{tabular}{l@{~}|l@{~}|r@{~}r@{~}|r@{~}l}
          &        & \multicolumn{2}{c|}{Time to}  &\\
          &        & \multicolumn{2}{c|}{resolve} &\\
Injection & Removal& initial & now & \multicolumn{2}{l}{Scale up w.r.t initial} \\\hline
Phase $i$&Phase $i$ & 1 & 1 & 1 & ** \\
Phase $i$ &Phase $i+1$ & 1 & 2& 2 & **** \\
Phase $i$ &Phase $i+2$ & 1 & 2& 2 & **** \\
Phase $i$ &Phase $i+3$ & 1 & 2 & 2 & **** \\
 Phase $i$  &Phase $i+4$ &1 & 2& 2 &**** 
  \end{tabular}\end{center}
 \caption{Projects data consistent with
 the phase delay effect of \fig{curve2}.}\label{fig:curve2a}
\end{figure}
 
Beck speculated that there might be another kind of project, shown in \fig{curve2},
that does no exhibit phase delay. In this next quote, Beck clearly states
the importance of avoiding phase delay (in fact, for Beck, it is {\em the} major
motivation for his work):
\begin{quote}
This (\fig{curve2}) is one of the premises of (agile). It is the technical premise of (agile). If the cost of change rose slowly over time, you would act completely differently from how you do under the assumption that costs rise exponentially. You would make big decisions as late in the process as possible, to defer the cost of making the decisions and to have the greatest possible chance that they would be right. You would only implement what you had to, in hopes that the needs you anticipate for tomorrow wouldn't come true. You would introduce elements to the design only as they simplified existing code or made writing the next bit of code simpler.~\cite{beck00}
\end{quote}
Beck makes one other comment that is relevant to this paper: 
\begin{quote}
The software development community has spent enormous resources in recent decades trying to reduce the cost of change-better languages, better database technology, better programming practices, better environments and tools, new notations.

What would we do if all that investment paid off? What if all that work on languages and databases and whatnot actually got somewhere? What if the cost of change didn't rise exponentially over time?~\cite{beck00}
\end{quote}
The results of this paper are that, in our sample of hundreds of projects, there is no evidence for phase delay.
This result is   consistent with Beck being correct: all that work on    languages and databases
have paid off and, hence, modern software projects do not suffer from the  phase delay effect.  

XXX clumsy link here
That said, where our results
diverge from Beck is that we have observed this lack of phase delay in  projects that are not agile. Those
projects are described below.

\section{Related Work}
 

\input{lucasRelatedWork}

\section{A Survey on Phase Delay and Cost-to-fix Belief}
\label{survey}

\input{lucas}





%\section{carterCharts}

%\input{carter}

 
\begin{figure*}[!t]
\begin{center}
\includegraphics[width=6in]{waterfall3.png}
\end{center}
\caption{Different approaches to software development:  waterfall and agile (bottom left).}
\label{fig:waterfall}
\end{figure*}

\section{Data Collection}


\input{data}









\section{Analysis of Phase Delay in 171 TSP Projects}

\todo{
\bi
    \item What is the cost-to-fix curve in TSP data? Check against H1 and compare to past literature.
    \item What is the effect of phase delay? Check against H2.
\ei
}


\subsection{TSP Project Cost-to-fix curve}

The distribution of defects found and fixed per phase is shown in Figure~\ref{fig:fix-phase-dist}. A high percentage of defects were found and fixed in the early phases, i.e., requirements, high level design, and design reviews and inspections. \todo{How does this compare evidence from other sources?}

The median time to fix a defect in each phase is shown in \fig{fix-time-per-phase}. \todo{Overlay median on Boehm's chart \fig{cost-to-fix}. How does it compare?}




\begin{figure}[!ht]
\begin{center}
\includegraphics[width=3.3in]{fix-phase-dist.png}
\end{center}
\caption{Distribution of defects found and fixed by phase.}
\label{fig:fix-phase-dist}
\end{figure}



\begin{figure}[!ht]
\begin{center}
\begin{tabular}{r|rrr|l}
  Sample&\multicolumn{3}{c|}{Percentiles}\\ 
size & 25th & 50th & 75th & Phase removed \\
\hline
120 & 3 & 3 & 18 & ReqtsInspect.dat \\
126 & 3 & 3 & 20 & ReqtsReview.dat \\
70 & 2 & 2 & 10 & HLDInspect.dat \\
 55 &    2 &    1&    7&HLDReview.dat \\

278&    7&    7&   33&DesignInspect.dat \\
233&    6&    6&   25&DesignReview.dat \\

315&    8&    7&   34&CodeInspect.dat \\ 
282&    7&    7&   32&CodeReview.dat\\ 

425&   11&   10&   54&UnitTest.dat \\ 
226&    7&    6&   46&IntTest.dat \\ 
194&    7&    6&   52&SysTest.dat\\ 
 88&    4&    3&   38&AcceptTest.dat\\
\end{tabular}
\end{center}
\caption{Median fix time per phase}
\label{fig:fix-time-per-phase}
\end{figure}

We reject $H_1$.

\subsection{Phase delay analysis}
\todo{
\bi
    \item What can we say about \fig{raw}
    \item What can we say about \fig{scale}
    \item What is the difference between \fig{raw} and \fig{scale}
\ei
}

In the TSP data, there is a weak correlation (Kendall's $\tau = 0.19, p < 0.01, 95\% CI$) between a defect's cost-to-fix (minutes of effort) and the defects phase delay (phase removed - phase injected). The correlations were also calculated for each phase, e.g., does the phase delay of defects found in Test correlate with the defect's cost-to-fix (Table~\ref{tab:phase_corr}). 

We reject $H_2$.




\begin{figure*}[!t] 
 \renewcommand{\baselinestretch}{0.7}
 \scriptsize
\begin{center}
\begin{tabular}{r|rrr|ll|rl}
  Sample&\multicolumn{3}{c|}{Percentiles}\\ 
size & 25th & 50th & 75th & Phase injected & Phase removed & \multicolumn{2}{l}{Scale up w.r.t. to first phase}\\\hline
\\
  29 &     1 &     3 &    12 & Before & CodeInspect  & 1.00  &  **  \\
  26 &     1 &     4 &     6 & Before & CodeReview  & 1.33  &  ***  \\
  73 &     5 &    13 &    59 & Before & UnitTest  & 4.33  &  *********  \\
  36 &     7 &    24 &    54 & Before & IntTest  & 8.00  &  ****************  \\
  34 &     8 &    28 &    56 & Before & SysTest  & 9.33  &  *******************  \\\hline
\\
128 &     2 &     6 &    15 & Reqts & ReqtsInspect  & 1.00  &  **  \\
132 &     2 &     5 &    12 & Reqts & ReqtsReview  & 0.83  &  **  \\
 28 &     3 &    13 &    29 & Reqts & DesignInspect  & 2.16  &  *****  \\
 22 &     6 &    14 &    31 & Reqts & DesignReview  & 2.33  &  *****  \\\hline
\\
272 &     6 &    14 &    30 & Design & DesignInspect  & 1.00  &  **  \\
 32 &     1 &     4 &    13 & Design & DesignInspection  & 0.28  &  *  \\
240 &     4 &    11 &    22 & Design & DesignReview  & 0.78  &  **  \\
110 &     3 &    11 &    23 & Design & CodeInspect  & 0.78  &  **  \\
 91 &     3 &     7 &    18 & Design & CodeReview  & 0.50  &  *  \\
323 &     8 &    21 &    45 & Design & UnitTest  & 1.50  &  ***  \\
 56 &     7 &    20 &    41 & Design & IntTest  & 1.42  &  ***  \\
 69 &     9 &    27 &    53 & Design & SysTest  & 1.92  &  ****  \\\hline
\\
310 &     6 &    15 &    31 & Code & CodeInspect  & 1.00  &  **  \\
285 &     5 &    14 &    29 & Code & CodeReview  & 0.93  &  **  \\
287 &     7 &    17 &    44 & Code & UnitTest  & 1.13  &  ***  \\
180 &     7 &    20 &    45 & Code & IntTest  & 1.33  &  ***  \\
137 &     6 &    15 &    35 & Code & SysTest  & 1.00  &  **  \\
 37 &     1 &    11 &    27 & Code & AcceptTest  & 0.73  &  **  \\

 \end{tabular}
\end{center}
\caption{Distribution of fix times seen in SEI TSP data.}
\label{fig:raw}
\end{figure*}





 
 \begin{figure}[!t]
\renewcommand{\baselinestretch}{0.7}
\scriptsize
\begin{center}
\begin{tabular}{l@{~~}|l@{~}|r@{~}|r@{~}r@{~}|r@{~}l}
           \multicolumn{2}{c}{~}                 &  &\multicolumn{2}{c|}{median}\\
  Injection&   Removal& $n$ & initial & now & \multicolumn{2}{l}{Scale up, w.r.t. initial}
\input{deltas}
\end{tabular}
\end{center}
\caption{50th percentile (median) scale ups  for  time to resolve issues (taken from \fig{raw}).}
\label{fig:scale}
\end{figure}
 



 
\begin{table}[ht]
\centering
\begin{tabular}{lll}
  Phase & $tau$ & $p$ \\ 
  \hline
BeforeDevelopment & NA & -- \\ 
  Planning & NA & -- \\ 
  Reqts & NA & -- \\ 
  ReqtsReview & 0.05 & 0.01 \\ 
  ReqtsInspect & 0.17 & $<$0.01 \\ 
  HLD & NA & -- \\ 
  HLDReview & 0.04 & 0.24 \\ 
  HLDInspect & -0.07 & 0.05 \\ 
  Design & 0.19 & 0.08 \\ 
  DesignReview & 0.08 & $<$0.01 \\ 
  DesignInspect & 0.06 & $<$0.01 \\ 
  Code & 0.21 & $<$0.01 \\ 
  CodeReview & 0.15 & $<$0.01 \\ 
  CodeInspect & 0.11 & $<$0.01 \\ 
  Test & 0.16 & $<$0.01 \\ 
  QualTest & NA & -- \\ 
  IntTest & 0.13 & $<$0.01 \\ 
  SysTest & 0.32 & $<$0.01 \\ 
  AcceptTest & 0.07 & 0.29 \\ 
  \end{tabular}
\caption{Correlation of cost-to-fix and phase delay for defects found in each phase (Kendall's $\tau$)} 
\label{tab:phase_corr}
\end{table}

\section{Phase Delay in Slower Bugs?}
In these results, we checked if a small number of bugs are most expensive. 
To check this:
\bi
\item
We repeated the analysis that generated \fig{scale},
but instead of looking at the 50th percentile, we displayed the scale up factors
\item 
We only
checked the Design and Code scale up results since, from \fig{scale}, it is clear these
have the most examples of longest phase delays.
\ei 
The  results are shown in \fig{scale90} and these
results are somewhat different for Design and Coding issues: 
\bi 
\item For Design issues,  these have the same
general form as the 50th percentile results. That is, while it it certainly faster
to remove thing sin the phase where they are created, once we leave that phase
it does not seem to matter much how many phases we wait before fixing the issue.
\item
\item For Coding issues, the seems little impact of phase delay on the time
required to fix.
\ei

 


\begin{figure}[!t]
\renewcommand{\baselinestretch}{0.7}
\scriptsize
\begin{center}
\begin{tabular}{l@{~~}|l@{~}|r@{~}|r@{~}r@{~}|r@{~}l}
          \multicolumn{2}{c}{~}                 &  &\multicolumn{2}{c|}{99th }\\
           \multicolumn{2}{c}{~}                 &  &\multicolumn{2}{c|}{percentile }\\
  Injection&   Removal& $n$ & initial & now & \multicolumn{2}{l}{Scale up, w.r.t. initial}
\input{deltasBigBig}
\end{tabular}
\end{center}
\caption{99th percentile scale ups.}
\label{fig:scale99}
\end{figure}

\begin{figure}[!t]
\renewcommand{\baselinestretch}{0.7}
\scriptsize
\begin{center}
\begin{tabular}{l@{~~}|l@{~}|r@{~}|r@{~}r@{~}|r@{~}l}
           \multicolumn{2}{c}{~}                 &  &\multicolumn{2}{c|}{mean}\\
  Injection&   Removal& $n$ & initial & now & \multicolumn{2}{l}{Scale up, w.r.t. initial}
\input{deltasMean}
\end{tabular}
\end{center}
\caption{Mean  scale ups.}
\label{fig:scale90}
\end{figure}


 


 

\section{Validity} 
A previous paper~\cite{shirai14} has applied a range of sanity checks to the data.
A common property of real-world data sets is the presence
of noisy entries (superfluous  or spurious data). 
The level of noise can be quite high. As reported
in \cite{shepperd12}, around
10\% to 30\%
of the records in the NASA MDP defect data sets are
affected by noise. Nichols et al.~\cite{shirai14}  report that
the noise levesl in the SEI TSE data are smaller than those seen
in other data sets. They found in the SEI TSP data that:\bi 
\item
4\% of the data was incorrect such as  null values of illegal formats;
\item  2\% of the data has inconsistencies such as timestamps
where the stop time was before the start time;
\item 3\% of the data had data that was not credible
such as tasks listed in one day that took like than six hours.
\ei 


XXX post release. really hard to change? auto-updates, package managers, web-based apps. 
 
\section{Related Work}


\begin{figure}
\begin{center}
\scriptsize\begin{tabular}{|l@{~:~}l|}\hline
Bug Prediction Dataset &http://bug.inf.usi.ch \\
Eclipse Bug Data &http://goo.gl/tYKahN \\
FLOSSMetrics& http://flossmetrics.org \\
FLOSSMole &http://flossmole.org \\
IBSBSG& http://www.isbsg.org \\
ohloh& http://www.openhub.net \\
PROMISE &http://promisedata.googlecode.com \\
Qualitas Corpus &http://qualitascorpus.com \\
Software Artifact Repository &http://sir.unl.edu \\
SourceForge Research Data &http://zerlot.cse.nd.edu \\
Sourcerer Project &http://sourcerer.ics.uci.edu \\
Tukutuku &http://www.metriq.biz/tukutuku \\
Ultimate Debian Database &http://udd.debian.org\\\hline
\end{tabular}
\end{center}
\caption{Some repositories of software engineering data.}\label{fig:sedata}
\end{figure}
The data collected in the SEI TSP databases contains 
extensive process details
including a detailed phased breakdown showing what happened at what
phases of the lifecycle. This makes this data somewhat different to the standard
 publicly available
software projects data sets currently available to 
SE researchers. Most of the \fig{sedata} data sets have software product information
such as full source code, or summaries of static features.
A subset of that data contain issue or defect reports and/or
the time taken to build these systems.  We are unaware
of any of these having detailed phase-by-phase breakdowns of project data.



\section*{Acknowledgements}
This work was partially funded by an National Science
Foundation grant NSF-CISE 1302169.

\clearpage
\vspace*{0.5mm}
\scriptsize
\bibliographystyle{plain}
\bibliography{refs} 


\end{document}


One way to address the absence of general laws is  {\em local learning}.
Even if general laws
do not exist, then {\em there exists general methods for finding the best local laws}.
For example, one of us (Nichols) works extensively with software projects to help
them tune their local software process in order to better fit their local needs.XXX.

 Recent studies report that better predictors of the properties of
software projects (effort and defects) can be generated by first clustering
(a.k.a. stratifying or contextualizing or localizing) the data then learning
different models for each cluster~\cite{posnett11,betten14,betta12,yang11,yang13,minku13}.
Those {\em local learning} approach finds better predictions with lower variance than
models learned from all the unclustered data~\cite{me12d,me11m}.

The problem with the local learning is that they it can generate conclusions
that conflict with strongly-held beliefs of the humans members of the software development team.
It is an open issue how to handle those conflicts since it is unwise to
always reject the conclusions of local learning  or the software developers.
The local learners may be wrong due to incorrect assumptions by the data scientists who configured the learners~\cite{me11e,shull02}.
On the other hand,  the humans may be wrong due to the cognitive biases described above.
Experienced data scientists use a cyclic approach where
(a)~the collected data; and (b)~the learning methods; and (c) the goals of the inquiry are matured  using
insight offered by humans as they study the
feedback generated by the learners~\cite{Fayyad96,me11e}.



the learners' conclusions are discussed with the team in or

(which may be erroneous
due to incorrect assumptions 


We come to this work since  recent results on {\em locality} by Menzies and others


XXXX In those 171 projects, we observed that it is
 fastest to fix issues within the same phase $i$ as when they are generated. But in a result that contradicts
 the phase delay effect, once an issue ``escapes'' phase $i$
 it so no more expensive to resolve an issue in phase $i+1$ than
 phase $i+2, i+3$, etc. Further, that increased effort may be quite modest.
 That is, contrary to established wisdom, once an  issue ``escapes'' a phase, there is no
 need to  rapidly retire that issue as soon as possible. 
Our conclusion discusses the implications for our field, and how we might better
propagated and monitor the wisdom of our field. 



\begin{figure}
\begin{center}
\begin{tabular}{rrl}
year& \# issues&\\\hline
2006 &  44 &\\
2007 &  34 &\\
2008&  288 &\rule{1mm}{2mm}\\
2009&  846 &\rule{3mm}{2mm}\\
2010& 1007 &\rule{3mm}{2mm}\\
2011& 3273 &\rule{10mm}{2mm}\\
2012&18102 &\rule{45mm}{2mm}\\
2013&20336 &\rule{50mm}{2mm}\\
2014& 3307 & \rule{10mm}{2mm}\\\cline{1-2}
Total:&47228
\end{tabular}
\end{center}
\caption{This paper studies 47,228 issues recorded 2006 to 2014.}\label{fig:years}
\end{figure}